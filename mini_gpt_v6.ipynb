{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps:0\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "# In macos, using mps:0\n",
    "# In windows,using cuda\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps:0\") # for MacBook\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TJ-bldfVYOB",
    "outputId": "763a242d-c830-4801-9733-7093caf9554e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-24 23:55:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：1115394 (1.1M) [text/plain]\n",
      "正在保存至: “input.txt.11”\n",
      "\n",
      "input.txt.11        100%[===================>]   1.06M  3.49MB/s  用时 0.3s      \n",
      "\n",
      "2024-03-24 23:55:07 (3.49 MB/s) - 已保存 “input.txt.11” [1115394/1115394])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "# Data loading\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Reading the database file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "\n",
    "print('vocab:', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try three tokenization methods on the data set.  Firstly, we use naive tokenrization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPkoI3EvV0dl"
   },
   "outputs": [],
   "source": [
    "# Naive Tokenrization\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use Byte Pair Encoding (BPE) method without and with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6c8Xbyg-t48",
    "outputId": "e59bb8e6-5926-4067-aa54-f8457a7b12b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (116, 104) into a new token 257\n",
      "merging (116, 32) into a new token 258\n",
      "merging (115, 32) into a new token 259\n",
      "merging (100, 32) into a new token 260\n",
      "merging (44, 32) into a new token 261\n",
      "merging (111, 117) into a new token 262\n",
      "merging (101, 114) into a new token 263\n",
      "merging (105, 110) into a new token 264\n",
      "merging (121, 32) into a new token 265\n",
      "merging (97, 110) into a new token 266\n",
      "merging (58, 10) into a new token 267\n",
      "merging (111, 114) into a new token 268\n",
      "merging (111, 32) into a new token 269\n",
      "merging (101, 110) into a new token 270\n",
      "merging (10, 10) into a new token 271\n",
      "merging (97, 114) into a new token 272\n",
      "merging (32, 257) into a new token 273\n",
      "merging (111, 110) into a new token 274\n",
      "merging (108, 108) into a new token 275\n",
      "merging (104, 97) into a new token 276\n",
      "merging (44, 10) into a new token 277\n",
      "merging (46, 271) into a new token 278\n",
      "merging (105, 259) into a new token 279\n",
      "merging (101, 115) into a new token 280\n",
      "merging (121, 262) into a new token 281\n",
      "merging (32, 115) into a new token 282\n",
      "merging (116, 269) into a new token 283\n",
      "merging (266, 260) into a new token 284\n",
      "merging (111, 119) into a new token 285\n",
      "merging (101, 97) into a new token 286\n",
      "merging (32, 109) into a new token 287\n",
      "merging (32, 119) into a new token 288\n",
      "merging (111, 102) into a new token 289\n",
      "merging (32, 104) into a new token 290\n",
      "merging (264, 103) into a new token 291\n",
      "merging (111, 109) into a new token 292\n",
      "merging (32, 97) into a new token 293\n",
      "merging (99, 104) into a new token 294\n",
      "merging (257, 256) into a new token 295\n",
      "merging (115, 116) into a new token 296\n",
      "merging (32, 98) into a new token 297\n",
      "merging (110, 111) into a new token 298\n",
      "merging (105, 114) into a new token 299\n",
      "merging (102, 268) into a new token 300\n",
      "merging (118, 256) into a new token 301\n",
      "merging (101, 261) into a new token 302\n",
      "merging (105, 257) into a new token 303\n",
      "merging (273, 256) into a new token 304\n",
      "merging (115, 101) into a new token 305\n"
     ]
    }
   ],
   "source": [
    "# Tokenrization by BPE\n",
    "class BytePairEncoding:\n",
    "    def __init__(self, text, vocab_size=306):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        self.merges = {}\n",
    "        self._build_vocab(text)\n",
    "\n",
    "    def _get_stats(self, ids):\n",
    "        # Assuming get_stats function's implementation is provided elsewhere in your code\n",
    "        stats = {}\n",
    "        for i in range(len(ids)-1):\n",
    "            pair = (ids[i], ids[i+1])\n",
    "            if pair in stats:\n",
    "                stats[pair] += 1\n",
    "            else:\n",
    "                stats[pair] = 1\n",
    "        return stats\n",
    "\n",
    "\n",
    "\n",
    "    def merge(self, ids, pair, idx):\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "    def _build_vocab(self, text):\n",
    "        tokens = text.encode(\"utf-8\")\n",
    "        tokens = list(map(int, tokens))\n",
    "        num_merges = self.vocab_size - 256\n",
    "        ids = list(tokens)\n",
    "        for i in range(num_merges):\n",
    "            stats = self._get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            print(f\"merging {pair} into a new token {idx}\")\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "            self.merges[pair] = idx\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self._get_stats(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self.merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "bpe = BytePairEncoding(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMqgSwPx_6EZ",
    "outputId": "1d39ef77-5bc6-45e6-85f3-ba6769b6c070",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (116, 104) into a new token 257\n",
      "merging (116, 32) into a new token 258\n",
      "merging (115, 32) into a new token 259\n",
      "merging (100, 32) into a new token 260\n",
      "merging (44, 32) into a new token 261\n",
      "merging (111, 117) into a new token 262\n",
      "merging (101, 114) into a new token 263\n",
      "merging (105, 110) into a new token 264\n",
      "merging (121, 32) into a new token 265\n",
      "merging (97, 110) into a new token 266\n",
      "merging (58, 10) into a new token 267\n",
      "merging (111, 114) into a new token 268\n",
      "merging (111, 32) into a new token 269\n",
      "merging (101, 110) into a new token 270\n",
      "merging (10, 10) into a new token 271\n",
      "merging (97, 114) into a new token 272\n",
      "merging (32, 257) into a new token 273\n",
      "merging (111, 110) into a new token 274\n",
      "merging (108, 108) into a new token 275\n",
      "merging (104, 97) into a new token 276\n",
      "merging (44, 10) into a new token 277\n",
      "merging (46, 271) into a new token 278\n",
      "merging (105, 259) into a new token 279\n",
      "merging (101, 115) into a new token 280\n",
      "merging (121, 262) into a new token 281\n",
      "merging (32, 115) into a new token 282\n",
      "merging (116, 269) into a new token 283\n",
      "merging (266, 260) into a new token 284\n",
      "merging (111, 119) into a new token 285\n",
      "merging (101, 97) into a new token 286\n",
      "merging (32, 109) into a new token 287\n",
      "merging (32, 119) into a new token 288\n",
      "merging (111, 102) into a new token 289\n",
      "merging (32, 104) into a new token 290\n",
      "merging (264, 103) into a new token 291\n",
      "merging (111, 109) into a new token 292\n",
      "merging (32, 97) into a new token 293\n",
      "merging (99, 104) into a new token 294\n",
      "merging (257, 256) into a new token 295\n",
      "merging (115, 116) into a new token 296\n",
      "merging (32, 98) into a new token 297\n",
      "merging (110, 111) into a new token 298\n",
      "merging (105, 114) into a new token 299\n",
      "merging (102, 268) into a new token 300\n",
      "merging (118, 256) into a new token 301\n",
      "merging (101, 261) into a new token 302\n",
      "merging (105, 257) into a new token 303\n",
      "merging (273, 256) into a new token 304\n",
      "merging (115, 101) into a new token 305\n",
      "merging (108, 105) into a new token 306\n",
      "merging (84, 104) into a new token 307\n",
      "merging (275, 32) into a new token 308\n",
      "merging (114, 101) into a new token 309\n",
      "merging (115, 258) into a new token 310\n",
      "merging (97, 258) into a new token 311\n",
      "merging (65, 110) into a new token 312\n",
      "merging (73, 32) into a new token 313\n",
      "merging (101, 272) into a new token 314\n",
      "merging (105, 109) into a new token 315\n",
      "merging (105, 116) into a new token 316\n",
      "merging (111, 111) into a new token 317\n",
      "merging (103, 104) into a new token 318\n",
      "merging (97, 116) into a new token 319\n",
      "merging (105, 115) into a new token 320\n",
      "merging (108, 101) into a new token 321\n",
      "merging (263, 32) into a new token 322\n",
      "merging (262, 114) into a new token 323\n",
      "merging (312, 260) into a new token 324\n",
      "merging (39, 259) into a new token 325\n",
      "merging (101, 101) into a new token 326\n",
      "merging (298, 258) into a new token 327\n",
      "merging (109, 265) into a new token 328\n",
      "merging (59, 10) into a new token 329\n",
      "merging (114, 97) into a new token 330\n",
      "merging (46, 10) into a new token 331\n",
      "merging (281, 114) into a new token 332\n",
      "merging (117, 114) into a new token 333\n",
      "merging (276, 258) into a new token 334\n",
      "merging (114, 105) into a new token 335\n",
      "merging (117, 258) into a new token 336\n",
      "merging (108, 260) into a new token 337\n",
      "merging (289, 32) into a new token 338\n",
      "merging (79, 267) into a new token 339\n",
      "merging (101, 260) into a new token 340\n",
      "merging (108, 97) into a new token 341\n",
      "merging (105, 258) into a new token 342\n",
      "merging (114, 111) into a new token 343\n",
      "merging (263, 256) into a new token 344\n",
      "merging (101, 259) into a new token 345\n",
      "merging (100, 261) into a new token 346\n",
      "merging (117, 110) into a new token 347\n",
      "merging (69, 78) into a new token 348\n",
      "merging (107, 256) into a new token 349\n",
      "merging (121, 261) into a new token 350\n",
      "merging (73, 78) into a new token 351\n",
      "merging (32, 100) into a new token 352\n",
      "merging (63, 271) into a new token 353\n",
      "merging (97, 259) into a new token 354\n",
      "merging (102, 97) into a new token 355\n",
      "merging (119, 303) into a new token 356\n",
      "merging (276, 301) into a new token 357\n",
      "merging (83, 267) into a new token 358\n",
      "merging (32, 99) into a new token 359\n",
      "merging (87, 104) into a new token 360\n",
      "merging (257, 311) into a new token 361\n",
      "merging (270, 116) into a new token 362\n",
      "merging (257, 101) into a new token 363\n",
      "merging (99, 101) into a new token 364\n",
      "merging (115, 104) into a new token 365\n",
      "merging (109, 97) into a new token 366\n",
      "merging (32, 112) into a new token 367\n",
      "merging (257, 263) into a new token 368\n",
      "merging (98, 101) into a new token 369\n",
      "merging (46, 32) into a new token 370\n",
      "merging (65, 82) into a new token 371\n",
      "merging (99, 256) into a new token 372\n",
      "merging (291, 32) into a new token 373\n",
      "merging (97, 108) into a new token 374\n",
      "merging (59, 32) into a new token 375\n"
     ]
    }
   ],
   "source": [
    "# Tokenrization by BPE with regularization\n",
    "\n",
    "\n",
    "class BytePairEncoding:\n",
    "    def __init__(self, text, vocab_size=306):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        self.merges = {}\n",
    "        self.gpt2pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\") # Regularization\n",
    "        self._build_vocab(text)\n",
    "\n",
    "    def _get_stats(self, ids):\n",
    "        # Assuming get_stats function's implementation is provided elsewhere in your code\n",
    "        stats = {}\n",
    "        for i in range(len(ids)-1):\n",
    "            pair = (ids[i], ids[i+1])\n",
    "            if pair in stats:\n",
    "                stats[pair] += 1\n",
    "            else:\n",
    "                stats[pair] = 1\n",
    "        return stats\n",
    "\n",
    "\n",
    "\n",
    "    def merge(self, ids, pair, idx):\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        # Using regularization\n",
    "        tokens = regex.findall(self.gpt2pat, text)\n",
    "        # Transform tokens to utf-8\n",
    "        tokens = [token.encode('utf-8') for token in tokens]\n",
    "        flat_tokens = [int(byte) for token in tokens for byte in token]\n",
    "        return flat_tokens\n",
    "\n",
    "    def _build_vocab(self, text):\n",
    "        tokens = self._preprocess_text(text)\n",
    "        num_merges = self.vocab_size - 256\n",
    "        ids = list(tokens)\n",
    "        for i in range(num_merges):\n",
    "            stats = self._get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            print(f\"merging {pair} into a new token {idx}\")\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "            self.merges[pair] = idx\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self._preprocess_text(text)\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self._get_stats(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self.merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "bpe_re = BytePairEncoding(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eCS0ZFUTVA4Q"
   },
   "outputs": [],
   "source": [
    "# Split the data set\n",
    "\n",
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "\n",
    "\n",
    "    # Calculate split sizes\n",
    "    total_size = len(data)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size  # Ensure we use all data\n",
    "\n",
    "    # Split the data\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "# Split the data set into train_data, val_data, test_data\n",
    "data = torch.tensor(bpe.encode(text), dtype=torch.long)\n",
    "train_data, val_data, test_data = split_data(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-GPT architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
    "        self.ffwd = FeedFoward(n_embd,dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd, n_layer, n_head, block_size, dropout,vocab_size=306):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, batch_size, block_size, data):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data, batch_size, block_size)  \n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()  \n",
    "\n",
    "def train_model(config, max_iters = 5000):\n",
    "    \n",
    "   \n",
    "    model = MiniGPTModel(\n",
    "        n_embd=config['n_embd'],\n",
    "        n_head=config['n_head'],\n",
    "        n_layer=config['n_layer'],\n",
    "        block_size=config['block_size'],\n",
    "        dropout =config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    eval_interval = 100\n",
    "    eval_iters = 200\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Setting early-stopping\n",
    "    patience = 10  \n",
    "    patience_counter = 0  \n",
    "    best_val_loss = float('inf')  \n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        \n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses_train = estimate_loss(model,eval_iters, config['batch_size'], config['block_size'], train_data)\n",
    "            losses_val = estimate_loss(model,eval_iters, config['batch_size'], config['block_size'], val_data)\n",
    "            \n",
    "            print(f\"step {iter}: train loss {losses_train:.4f}, val loss {losses_val:.4f}\")\n",
    "            \n",
    "            train_losses.append((iter, losses_train))\n",
    "            val_losses.append((iter, losses_val))\n",
    "            \n",
    "\n",
    "            # check whether the val_loss improves\n",
    "            current_val_loss = losses_val\n",
    "            if current_val_loss < best_val_loss:\n",
    "                best_val_loss = current_val_loss\n",
    "                patience_counter = 0  \n",
    "            else:\n",
    "                patience_counter += 1  \n",
    "\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {iter + 1} iterations.\")\n",
    "            break\n",
    "            \n",
    "           \n",
    "        xb, yb = get_batch(train_data, config['batch_size'], config['block_size'])\n",
    "            \n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    return model, best_val_loss, train_losses, val_losses\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "fUFNy1hTScXt",
    "outputId": "c621f9c5-ead6-41b8-fa7e-bfea314c7962",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.240818 M parameters\n",
      "step 0: train loss 5.9291, val loss 5.9210\n",
      "step 100: train loss 4.1075, val loss 4.0993\n",
      "step 200: train loss 3.4831, val loss 3.5045\n",
      "step 300: train loss 3.2993, val loss 3.3361\n",
      "step 400: train loss 3.2133, val loss 3.2590\n",
      "step 500: train loss 3.1470, val loss 3.1898\n",
      "step 600: train loss 3.0737, val loss 3.1369\n",
      "step 700: train loss 3.0301, val loss 3.1103\n",
      "step 800: train loss 2.9613, val loss 3.0482\n",
      "step 900: train loss 2.9234, val loss 3.0105\n",
      "step 1000: train loss 2.8852, val loss 2.9758\n",
      "step 1100: train loss 2.8417, val loss 2.9367\n",
      "step 1200: train loss 2.7967, val loss 2.8969\n",
      "step 1300: train loss 2.7620, val loss 2.8969\n",
      "step 1400: train loss 2.7251, val loss 2.8516\n",
      "step 1500: train loss 2.6903, val loss 2.8259\n",
      "step 1600: train loss 2.6676, val loss 2.7966\n",
      "step 1700: train loss 2.6436, val loss 2.7733\n",
      "step 1800: train loss 2.6005, val loss 2.7517\n",
      "step 1900: train loss 2.5838, val loss 2.7427\n",
      "step 2000: train loss 2.5582, val loss 2.7270\n",
      "step 2100: train loss 2.5375, val loss 2.6952\n",
      "step 2200: train loss 2.5397, val loss 2.6761\n",
      "step 2300: train loss 2.5067, val loss 2.6652\n",
      "step 2400: train loss 2.4942, val loss 2.6670\n",
      "step 2500: train loss 2.4814, val loss 2.6626\n",
      "step 2600: train loss 2.4682, val loss 2.6279\n",
      "step 2700: train loss 2.4367, val loss 2.6309\n",
      "step 2800: train loss 2.4320, val loss 2.6061\n",
      "step 2900: train loss 2.4300, val loss 2.5969\n",
      "step 3000: train loss 2.4088, val loss 2.6015\n",
      "step 3100: train loss 2.3842, val loss 2.5515\n",
      "step 3200: train loss 2.3874, val loss 2.5550\n",
      "step 3300: train loss 2.3804, val loss 2.5602\n",
      "step 3400: train loss 2.3791, val loss 2.5399\n",
      "step 3500: train loss 2.3583, val loss 2.5376\n",
      "step 3600: train loss 2.3486, val loss 2.5390\n",
      "step 3700: train loss 2.3304, val loss 2.5221\n",
      "step 3800: train loss 2.3274, val loss 2.5111\n",
      "step 3900: train loss 2.3267, val loss 2.5069\n",
      "step 4000: train loss 2.3230, val loss 2.5071\n",
      "step 4100: train loss 2.3072, val loss 2.4728\n",
      "step 4200: train loss 2.3020, val loss 2.5006\n",
      "step 4300: train loss 2.3065, val loss 2.4815\n",
      "step 4400: train loss 2.2939, val loss 2.4695\n",
      "step 4500: train loss 2.2979, val loss 2.4694\n",
      "step 4600: train loss 2.2710, val loss 2.4636\n",
      "step 4700: train loss 2.2997, val loss 2.4854\n",
      "step 4800: train loss 2.2844, val loss 2.4623\n",
      "step 4900: train loss 2.2646, val loss 2.4521\n",
      "step 4999: train loss 2.2684, val loss 2.4701\n",
      "\u0000lone soo, enders:\n",
      "Ging lin thou werd'd; but are in him this diest me frought\n",
      "And seisting her.\n",
      "\n",
      "FLORIZEL:\n",
      "It scontale, as are too sues feet spoken'd\n",
      "In nows and when I'll to-thee reborn at the stay.\n",
      "\n",
      "ROMEO:\n",
      "Stike a himself with hences! if to;'\n",
      "Or may soul shall. Cousins of my too maight\n",
      "Against thou gring an unsurpovet, bid me and honour's lost,\n",
      "That thought use se'ter won endrators and bown,-e\n",
      "off those breing them mother'ding sue! very falest, comes home;\n",
      "No no my grace in . Which whiled father: gollaith hight;\n",
      "Shall for tishing beaut titness of his need etwe?\n",
      "\n",
      "First Meniale:\n",
      "Cusine! and sun, in the forch the where of our to our hight,\n",
      "Reechant well as retistice, and slife to-freen more of slister, no,\n",
      "Therself the infed defter's grace of son her,\n",
      "Must batter-rabber!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I Reaver you from Rount Bohhamne dish;\n",
      "He is is with questortaars net you.\n",
      "\n",
      "Mestie:\n",
      "The is ches or thy lights, and my none see ould sarle.\n",
      "\n",
      "Nurse:\n",
      "I fots one to it is away so-the ches plead were a great lie.\n",
      "\n",
      "LARTINGS:\n",
      "I wert hish and stam; women.\n",
      "\n",
      "KING EDW\n",
      "Into Henereford' wans. Whose my intod,\n",
      "Apore, found things and cother of holdiency!\n",
      "What him this earth, by and look'd, with life,\n",
      "Is do, or sometring your abraised of your away?\n",
      "A mine such, advers brack my Anseo unatord.\n",
      "\n",
      "BREGORY:\n",
      "Then is her of at plated or thyself?\n",
      "That you gentle thou my tends, no it siss of last on: I his diely\n",
      "this batted, thoughbousage that feet a profount it; he'y royated in the more.\n",
      "\n",
      "MENENIUS:\n",
      "Thea, he patiend; and, no should the oldiling diss-love,\n",
      "'I by hele a consurefortue spirity withn,\n",
      "Which of her we us proud cast the some, sue\n",
      "Henry hearty than you to bre, furthere; and this grivefy and the\n",
      "Revertan's not lig contule.\n",
      "\n",
      "SICINIUS:\n",
      "Hy thought, which you to seen near;\n",
      "To see a vair pressious set fretche aperon;\n",
      "But o'ath did of after there out of the world:\n",
      "Nor weet as compass thine againt,\n",
      "Some not, to all her sick of am to live a beest,\n",
      "To har not seem.\n",
      "\n",
      "JULIET:\n",
      "How feise wits will be is gentlemon?\n",
      "\n",
      "Nurse, who, but I am hanking! thus descared,\n",
      "And all giving server.\n",
      "\n",
      "How a griess,\n",
      "Yo libertying wept my cour sconsersed their from a dressof king,\n",
      "And rather's news on blight\n",
      "As thare mong holy yea largy.\n",
      "\n",
      "PAULINA:\n",
      "Too Gre, for thou fresh, in these of swit to opfort your bidt;\n",
      "Ever Englastionabthing those of eye\n",
      "Dords cousurses do, fort, thy hast not?\n",
      "Be thou lay us nife are, my deed; youis,\n",
      "Arrather's mayor with him with will am bless of that\n",
      "Cannot for whose lay heart, cheek of my\n",
      "To is live, and earth affar of yours.\n",
      "\n",
      "Clown:\n",
      "Before give more thy botter towarded distarkines, down service,\n",
      "The no more griet the quotaborn'st that\n",
      "For weeple hour, and sweet now,\n",
      "Thembering his gods is hines; you what is so. I, limb.\n",
      "In stPule may sitice of king of thiserless of lip, Frances.\n",
      "\n",
      "KING Rome crettest:\n",
      "And in the deeplarised; and his reashy, count expected obhard thought night\n",
      "Brures the gation's suppicious; but will are and suffe.\n",
      "\n",
      "QUETER:\n",
      "Who corr\n"
     ]
    }
   ],
   "source": [
    "# Train the model and check what generate\n",
    "# hyperparameters\n",
    "\n",
    "config_1 = {\n",
    "    'n_embd': 64,\n",
    "    'n_head': 4,\n",
    "    'n_layer': 4,\n",
    "    'block_size': 32,\n",
    "    'dropout': 0.0,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "  \n",
    "m, _, _, _ = train_model(config_1)\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(bpe.decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "lyohzxHRHo_i",
    "outputId": "2839ca34-3598-456e-841a-2bfcd049ae8d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing config: {'batch_size': 16, 'block_size': 16, 'learning_rate': 0.0001, 'n_embd': 288, 'n_head': 6, 'n_layer': 4, 'dropout': 0.3}\n",
      "step 0: train loss 5.9302, val loss 5.9261\n",
      "step 100: train loss 4.3197, val loss 4.3223\n",
      "step 200: train loss 3.8575, val loss 3.8858\n",
      "step 300: train loss 3.5220, val loss 3.5567\n",
      "step 400: train loss 3.3891, val loss 3.4147\n",
      "step 500: train loss 3.2996, val loss 3.3318\n",
      "step 600: train loss 3.2421, val loss 3.2923\n",
      "step 700: train loss 3.1930, val loss 3.2732\n",
      "step 800: train loss 3.1528, val loss 3.2011\n",
      "step 900: train loss 3.1098, val loss 3.1862\n",
      "step 1000: train loss 3.0815, val loss 3.1538\n",
      "step 1100: train loss 3.0538, val loss 3.1146\n",
      "step 1200: train loss 3.0248, val loss 3.0792\n",
      "step 1300: train loss 3.0093, val loss 3.0692\n",
      "step 1400: train loss 2.9908, val loss 3.0522\n",
      "step 1500: train loss 2.9716, val loss 3.0495\n",
      "step 1600: train loss 2.9533, val loss 3.0288\n",
      "step 1700: train loss 2.9276, val loss 3.0171\n",
      "step 1800: train loss 2.9130, val loss 2.9842\n",
      "step 1900: train loss 2.8955, val loss 2.9921\n",
      "step 2000: train loss 2.8903, val loss 2.9599\n",
      "step 2100: train loss 2.8428, val loss 2.9430\n",
      "step 2200: train loss 2.8553, val loss 2.9350\n",
      "step 2300: train loss 2.8439, val loss 2.9044\n",
      "step 2400: train loss 2.8179, val loss 2.9135\n",
      "step 2500: train loss 2.7834, val loss 2.8828\n",
      "step 2600: train loss 2.7989, val loss 2.8817\n",
      "step 2700: train loss 2.7623, val loss 2.8621\n",
      "step 2800: train loss 2.7605, val loss 2.8514\n",
      "step 2900: train loss 2.7316, val loss 2.8421\n",
      "step 3000: train loss 2.7411, val loss 2.8358\n",
      "step 3100: train loss 2.7121, val loss 2.8293\n",
      "step 3200: train loss 2.7211, val loss 2.8145\n",
      "step 3300: train loss 2.7219, val loss 2.8315\n",
      "step 3400: train loss 2.6884, val loss 2.8043\n",
      "step 3500: train loss 2.6737, val loss 2.7869\n",
      "step 3600: train loss 2.6588, val loss 2.7933\n",
      "step 3700: train loss 2.6543, val loss 2.7846\n",
      "step 3800: train loss 2.6402, val loss 2.7810\n",
      "step 3900: train loss 2.6256, val loss 2.7587\n",
      "step 4000: train loss 2.6326, val loss 2.7574\n",
      "step 4100: train loss 2.6153, val loss 2.7569\n",
      "step 4200: train loss 2.6226, val loss 2.7283\n",
      "step 4300: train loss 2.6200, val loss 2.7299\n",
      "step 4400: train loss 2.5878, val loss 2.7214\n",
      "step 4500: train loss 2.5907, val loss 2.7204\n",
      "step 4600: train loss 2.5759, val loss 2.7106\n",
      "step 4700: train loss 2.5658, val loss 2.7295\n",
      "step 4800: train loss 2.5638, val loss 2.6926\n",
      "step 4900: train loss 2.5584, val loss 2.6776\n",
      "step 4999: train loss 2.5515, val loss 2.7128\n",
      "New best config: {'batch_size': 16, 'block_size': 16, 'learning_rate': 0.0001, 'n_embd': 288, 'n_head': 6, 'n_layer': 4, 'dropout': 0.3} with loss: 2.677591323852539\n",
      "Testing config: {'batch_size': 16, 'block_size': 64, 'learning_rate': 0.0001, 'n_embd': 288, 'n_head': 6, 'n_layer': 4, 'dropout': 0.2}\n",
      "step 0: train loss 5.8861, val loss 5.8852\n",
      "step 100: train loss 4.2048, val loss 4.2061\n",
      "step 200: train loss 3.5617, val loss 3.5917\n",
      "step 300: train loss 3.3518, val loss 3.3821\n",
      "step 400: train loss 3.2619, val loss 3.2999\n",
      "step 500: train loss 3.2027, val loss 3.2470\n",
      "step 600: train loss 3.1621, val loss 3.2104\n",
      "step 700: train loss 3.1301, val loss 3.1802\n",
      "step 800: train loss 3.1071, val loss 3.1648\n",
      "step 900: train loss 3.0781, val loss 3.1354\n",
      "step 1000: train loss 3.0469, val loss 3.1107\n",
      "step 1100: train loss 3.0269, val loss 3.0939\n",
      "step 1200: train loss 2.9889, val loss 3.0653\n",
      "step 1300: train loss 2.9595, val loss 3.0385\n",
      "step 1400: train loss 2.9479, val loss 3.0253\n",
      "step 1500: train loss 2.9073, val loss 3.0027\n",
      "step 1600: train loss 2.8981, val loss 2.9792\n",
      "step 1700: train loss 2.8646, val loss 2.9596\n",
      "step 1800: train loss 2.8508, val loss 2.9397\n",
      "step 1900: train loss 2.8143, val loss 2.9159\n",
      "step 2000: train loss 2.7766, val loss 2.8940\n",
      "step 2100: train loss 2.7645, val loss 2.8684\n",
      "step 2200: train loss 2.7378, val loss 2.8474\n",
      "step 2300: train loss 2.7135, val loss 2.8181\n",
      "step 2400: train loss 2.6888, val loss 2.8108\n",
      "step 2500: train loss 2.6597, val loss 2.7874\n",
      "step 2600: train loss 2.6386, val loss 2.7729\n",
      "step 2700: train loss 2.6306, val loss 2.7468\n",
      "step 2800: train loss 2.6046, val loss 2.7265\n",
      "step 2900: train loss 2.5840, val loss 2.7160\n",
      "step 3000: train loss 2.5712, val loss 2.7008\n",
      "step 3100: train loss 2.5441, val loss 2.6997\n",
      "step 3200: train loss 2.5195, val loss 2.6684\n",
      "step 3300: train loss 2.5118, val loss 2.6626\n",
      "step 3400: train loss 2.5028, val loss 2.6580\n",
      "step 3500: train loss 2.4945, val loss 2.6296\n",
      "step 3600: train loss 2.4683, val loss 2.6165\n",
      "step 3700: train loss 2.4628, val loss 2.6135\n",
      "step 3800: train loss 2.4389, val loss 2.5950\n",
      "step 3900: train loss 2.4297, val loss 2.5809\n",
      "step 4000: train loss 2.4050, val loss 2.5678\n",
      "step 4100: train loss 2.4025, val loss 2.5593\n",
      "step 4200: train loss 2.3911, val loss 2.5615\n",
      "step 4300: train loss 2.3805, val loss 2.5495\n",
      "step 4400: train loss 2.3646, val loss 2.5382\n",
      "step 4500: train loss 2.3578, val loss 2.5425\n",
      "step 4600: train loss 2.3477, val loss 2.5312\n",
      "step 4700: train loss 2.3461, val loss 2.5123\n",
      "step 4800: train loss 2.3325, val loss 2.5043\n",
      "step 4900: train loss 2.3292, val loss 2.5106\n",
      "step 4999: train loss 2.3170, val loss 2.4945\n",
      "New best config: {'batch_size': 16, 'block_size': 64, 'learning_rate': 0.0001, 'n_embd': 288, 'n_head': 6, 'n_layer': 4, 'dropout': 0.2} with loss: 2.494546413421631\n",
      "Testing config: {'batch_size': 32, 'block_size': 64, 'learning_rate': 1e-05, 'n_embd': 576, 'n_head': 4, 'n_layer': 8, 'dropout': 0.1}\n",
      "step 0: train loss 5.8949, val loss 5.8850\n",
      "step 100: train loss 4.4873, val loss 4.4818\n",
      "step 200: train loss 4.1610, val loss 4.1693\n",
      "step 300: train loss 3.7824, val loss 3.8032\n",
      "step 400: train loss 3.5550, val loss 3.5789\n",
      "step 500: train loss 3.4333, val loss 3.4592\n",
      "step 600: train loss 3.3574, val loss 3.3960\n",
      "step 700: train loss 3.3055, val loss 3.3448\n",
      "step 800: train loss 3.2657, val loss 3.3048\n",
      "step 900: train loss 3.2290, val loss 3.2800\n",
      "step 1000: train loss 3.2019, val loss 3.2479\n",
      "step 1100: train loss 3.1766, val loss 3.2291\n",
      "step 1200: train loss 3.1536, val loss 3.2028\n",
      "step 1300: train loss 3.1385, val loss 3.1908\n",
      "step 1400: train loss 3.1183, val loss 3.1809\n",
      "step 1500: train loss 3.1020, val loss 3.1616\n",
      "step 1600: train loss 3.0867, val loss 3.1457\n",
      "step 1700: train loss 3.0666, val loss 3.1328\n",
      "step 1800: train loss 3.0549, val loss 3.1232\n",
      "step 1900: train loss 3.0426, val loss 3.1036\n",
      "step 2000: train loss 3.0174, val loss 3.0949\n",
      "step 2100: train loss 3.0112, val loss 3.0880\n",
      "step 2200: train loss 2.9941, val loss 3.0658\n",
      "step 2300: train loss 2.9795, val loss 3.0500\n",
      "step 2400: train loss 2.9508, val loss 3.0352\n",
      "step 2500: train loss 2.9291, val loss 3.0169\n",
      "step 2600: train loss 2.9176, val loss 3.0023\n",
      "step 2700: train loss 2.8958, val loss 2.9949\n",
      "step 2800: train loss 2.8857, val loss 2.9772\n",
      "step 2900: train loss 2.8626, val loss 2.9610\n",
      "step 3000: train loss 2.8447, val loss 2.9433\n",
      "step 3100: train loss 2.8335, val loss 2.9288\n",
      "step 3200: train loss 2.8211, val loss 2.9241\n",
      "step 3300: train loss 2.8019, val loss 2.9111\n",
      "step 3400: train loss 2.7869, val loss 2.8931\n",
      "step 3500: train loss 2.7717, val loss 2.8774\n",
      "step 3600: train loss 2.7482, val loss 2.8594\n",
      "step 3700: train loss 2.7371, val loss 2.8468\n",
      "step 3800: train loss 2.7260, val loss 2.8332\n",
      "step 3900: train loss 2.7033, val loss 2.8207\n",
      "step 4000: train loss 2.6843, val loss 2.8091\n",
      "step 4100: train loss 2.6825, val loss 2.7966\n",
      "step 4200: train loss 2.6544, val loss 2.7796\n",
      "step 4300: train loss 2.6425, val loss 2.7673\n",
      "step 4400: train loss 2.6205, val loss 2.7611\n",
      "step 4500: train loss 2.6150, val loss 2.7487\n",
      "step 4600: train loss 2.6055, val loss 2.7357\n",
      "step 4700: train loss 2.5928, val loss 2.7349\n",
      "step 4800: train loss 2.5811, val loss 2.7199\n",
      "step 4900: train loss 2.5690, val loss 2.7072\n",
      "step 4999: train loss 2.5496, val loss 2.6961\n",
      "Testing config: {'batch_size': 16, 'block_size': 64, 'learning_rate': 0.001, 'n_embd': 288, 'n_head': 6, 'n_layer': 6, 'dropout': 0.2}\n",
      "step 0: train loss 5.8830, val loss 5.8828\n",
      "step 100: train loss 3.2444, val loss 3.3040\n",
      "step 200: train loss 3.1038, val loss 3.1683\n",
      "step 300: train loss 2.9072, val loss 2.9973\n",
      "step 400: train loss 2.7498, val loss 2.8783\n",
      "step 500: train loss 2.6213, val loss 2.7641\n",
      "step 600: train loss 2.5374, val loss 2.7036\n",
      "step 700: train loss 2.4704, val loss 2.6293\n",
      "step 800: train loss 2.4093, val loss 2.6012\n",
      "step 900: train loss 2.3781, val loss 2.5908\n",
      "step 1000: train loss 2.3388, val loss 2.5410\n",
      "step 1100: train loss 2.2950, val loss 2.4959\n",
      "step 1200: train loss 2.2645, val loss 2.4736\n",
      "step 1300: train loss 2.2491, val loss 2.4706\n",
      "step 1400: train loss 2.2299, val loss 2.4639\n",
      "step 1500: train loss 2.2035, val loss 2.4205\n",
      "step 1600: train loss 2.1664, val loss 2.3958\n",
      "step 1700: train loss 2.1487, val loss 2.3760\n",
      "step 1800: train loss 2.1458, val loss 2.3702\n",
      "step 1900: train loss 2.1258, val loss 2.3503\n",
      "step 2000: train loss 2.1231, val loss 2.3387\n",
      "step 2100: train loss 2.0884, val loss 2.3253\n",
      "step 2200: train loss 2.0762, val loss 2.3345\n",
      "step 2300: train loss 2.0804, val loss 2.3369\n",
      "step 2400: train loss 2.0602, val loss 2.3122\n",
      "step 2500: train loss 2.0515, val loss 2.3196\n",
      "step 2600: train loss 2.0482, val loss 2.2971\n",
      "step 2700: train loss 2.0255, val loss 2.2834\n",
      "step 2800: train loss 2.0212, val loss 2.2964\n",
      "step 2900: train loss 2.0083, val loss 2.2581\n",
      "step 3000: train loss 1.9975, val loss 2.2650\n",
      "step 3100: train loss 1.9990, val loss 2.2685\n",
      "step 3200: train loss 1.9937, val loss 2.2500\n",
      "step 3300: train loss 1.9886, val loss 2.2312\n",
      "step 3400: train loss 1.9697, val loss 2.2367\n",
      "step 3500: train loss 1.9668, val loss 2.2321\n",
      "step 3600: train loss 1.9665, val loss 2.2486\n",
      "step 3700: train loss 1.9583, val loss 2.2282\n",
      "step 3800: train loss 1.9548, val loss 2.2391\n",
      "step 3900: train loss 1.9547, val loss 2.2104\n",
      "step 4000: train loss 1.9330, val loss 2.2233\n",
      "step 4100: train loss 1.9277, val loss 2.2017\n",
      "step 4200: train loss 1.9144, val loss 2.1988\n",
      "step 4300: train loss 1.9123, val loss 2.2074\n",
      "step 4400: train loss 1.9102, val loss 2.1792\n",
      "step 4500: train loss 1.9040, val loss 2.2026\n",
      "step 4600: train loss 1.9034, val loss 2.1917\n",
      "step 4700: train loss 1.8958, val loss 2.2065\n",
      "step 4800: train loss 1.8933, val loss 2.2097\n",
      "step 4900: train loss 1.8858, val loss 2.1967\n",
      "step 4999: train loss 1.8911, val loss 2.1724\n",
      "New best config: {'batch_size': 16, 'block_size': 64, 'learning_rate': 0.001, 'n_embd': 288, 'n_head': 6, 'n_layer': 6, 'dropout': 0.2} with loss: 2.1723711490631104\n",
      "Testing config: {'batch_size': 32, 'block_size': 64, 'learning_rate': 0.0001, 'n_embd': 144, 'n_head': 4, 'n_layer': 4, 'dropout': 0.2}\n",
      "step 0: train loss 5.9041, val loss 5.9108\n",
      "step 100: train loss 4.4571, val loss 4.4543\n",
      "step 200: train loss 4.1918, val loss 4.2022\n",
      "step 300: train loss 3.7720, val loss 3.7960\n",
      "step 400: train loss 3.5318, val loss 3.5628\n",
      "step 500: train loss 3.4082, val loss 3.4451\n",
      "step 600: train loss 3.3343, val loss 3.3691\n",
      "step 700: train loss 3.2786, val loss 3.3180\n",
      "step 800: train loss 3.2340, val loss 3.2775\n",
      "step 900: train loss 3.2055, val loss 3.2529\n",
      "step 1000: train loss 3.1792, val loss 3.2289\n",
      "step 1100: train loss 3.1543, val loss 3.2073\n",
      "step 1200: train loss 3.1279, val loss 3.1888\n",
      "step 1300: train loss 3.1105, val loss 3.1697\n",
      "step 1400: train loss 3.0891, val loss 3.1496\n",
      "step 1500: train loss 3.0751, val loss 3.1385\n",
      "step 1600: train loss 3.0484, val loss 3.1141\n",
      "step 1700: train loss 3.0351, val loss 3.1034\n",
      "step 1800: train loss 3.0161, val loss 3.0889\n",
      "step 1900: train loss 2.9945, val loss 3.0730\n",
      "step 2000: train loss 2.9813, val loss 3.0613\n",
      "step 2100: train loss 2.9537, val loss 3.0397\n",
      "step 2200: train loss 2.9452, val loss 3.0235\n",
      "step 2300: train loss 2.9244, val loss 3.0097\n",
      "step 2400: train loss 2.9071, val loss 2.9918\n",
      "step 2500: train loss 2.8959, val loss 2.9778\n",
      "step 2600: train loss 2.8718, val loss 2.9615\n",
      "step 2700: train loss 2.8554, val loss 2.9421\n",
      "step 2800: train loss 2.8401, val loss 2.9323\n",
      "step 2900: train loss 2.8229, val loss 2.9199\n",
      "step 3000: train loss 2.8054, val loss 2.8947\n",
      "step 3100: train loss 2.7885, val loss 2.8884\n",
      "step 3200: train loss 2.7724, val loss 2.8730\n",
      "step 3300: train loss 2.7553, val loss 2.8612\n",
      "step 3400: train loss 2.7398, val loss 2.8460\n",
      "step 3500: train loss 2.7239, val loss 2.8290\n",
      "step 3600: train loss 2.7064, val loss 2.8106\n",
      "step 3700: train loss 2.6935, val loss 2.8108\n",
      "step 3800: train loss 2.6880, val loss 2.7935\n",
      "step 3900: train loss 2.6679, val loss 2.7747\n",
      "step 4000: train loss 2.6614, val loss 2.7735\n",
      "step 4100: train loss 2.6443, val loss 2.7642\n",
      "step 4200: train loss 2.6270, val loss 2.7587\n",
      "step 4300: train loss 2.6202, val loss 2.7378\n",
      "step 4400: train loss 2.5999, val loss 2.7247\n",
      "step 4500: train loss 2.5932, val loss 2.7221\n",
      "step 4600: train loss 2.5830, val loss 2.7114\n",
      "step 4700: train loss 2.5688, val loss 2.7049\n",
      "step 4800: train loss 2.5649, val loss 2.6975\n",
      "step 4900: train loss 2.5503, val loss 2.6950\n",
      "step 4999: train loss 2.5434, val loss 2.6708\n",
      "Testing config: {'batch_size': 64, 'block_size': 64, 'learning_rate': 1e-05, 'n_embd': 576, 'n_head': 4, 'n_layer': 8, 'dropout': 0.2}\n",
      "step 0: train loss 5.8534, val loss 5.8661\n",
      "step 100: train loss 4.4499, val loss 4.4558\n",
      "step 200: train loss 4.0605, val loss 4.0825\n",
      "step 300: train loss 3.6713, val loss 3.7091\n",
      "step 400: train loss 3.4769, val loss 3.5140\n",
      "step 500: train loss 3.3718, val loss 3.4171\n",
      "step 600: train loss 3.3083, val loss 3.3541\n",
      "step 700: train loss 3.2623, val loss 3.3068\n",
      "step 800: train loss 3.2244, val loss 3.2752\n",
      "step 900: train loss 3.1904, val loss 3.2457\n",
      "step 1000: train loss 3.1599, val loss 3.2198\n",
      "step 1100: train loss 3.1400, val loss 3.1920\n",
      "step 1200: train loss 3.1176, val loss 3.1774\n",
      "step 1300: train loss 3.0979, val loss 3.1605\n",
      "step 1400: train loss 3.0760, val loss 3.1404\n",
      "step 1500: train loss 3.0605, val loss 3.1262\n",
      "step 1600: train loss 3.0405, val loss 3.1125\n",
      "step 1700: train loss 3.0212, val loss 3.0950\n",
      "step 1800: train loss 3.0021, val loss 3.0738\n",
      "step 1900: train loss 2.9757, val loss 3.0551\n",
      "step 2000: train loss 2.9546, val loss 3.0380\n",
      "step 2100: train loss 2.9293, val loss 3.0109\n",
      "step 2200: train loss 2.9081, val loss 2.9947\n",
      "step 2300: train loss 2.8882, val loss 2.9768\n",
      "step 2400: train loss 2.8676, val loss 2.9624\n",
      "step 2500: train loss 2.8473, val loss 2.9465\n",
      "step 2600: train loss 2.8267, val loss 2.9257\n",
      "step 2700: train loss 2.8073, val loss 2.9047\n",
      "step 2800: train loss 2.7838, val loss 2.8850\n",
      "step 2900: train loss 2.7652, val loss 2.8756\n",
      "step 3000: train loss 2.7460, val loss 2.8590\n",
      "step 3100: train loss 2.7250, val loss 2.8413\n",
      "step 3200: train loss 2.7058, val loss 2.8320\n",
      "step 3300: train loss 2.6874, val loss 2.8088\n",
      "step 3400: train loss 2.6662, val loss 2.7951\n",
      "step 3500: train loss 2.6485, val loss 2.7767\n",
      "step 3600: train loss 2.6347, val loss 2.7654\n",
      "step 3700: train loss 2.6168, val loss 2.7540\n",
      "step 3800: train loss 2.6015, val loss 2.7380\n",
      "step 3900: train loss 2.5883, val loss 2.7280\n",
      "step 4000: train loss 2.5692, val loss 2.7170\n",
      "step 4100: train loss 2.5540, val loss 2.7025\n",
      "step 4200: train loss 2.5398, val loss 2.6906\n",
      "step 4300: train loss 2.5317, val loss 2.6765\n",
      "step 4400: train loss 2.5136, val loss 2.6695\n",
      "step 4500: train loss 2.5049, val loss 2.6667\n",
      "step 4600: train loss 2.4919, val loss 2.6482\n",
      "step 4700: train loss 2.4800, val loss 2.6409\n",
      "step 4800: train loss 2.4702, val loss 2.6304\n",
      "step 4900: train loss 2.4573, val loss 2.6205\n",
      "step 4999: train loss 2.4437, val loss 2.6089\n",
      "Testing config: {'batch_size': 16, 'block_size': 64, 'learning_rate': 0.0001, 'n_embd': 144, 'n_head': 4, 'n_layer': 8, 'dropout': 0.3}\n",
      "step 0: train loss 5.8737, val loss 5.8783\n",
      "step 100: train loss 4.4624, val loss 4.4625\n",
      "step 200: train loss 4.0721, val loss 4.0883\n",
      "step 300: train loss 3.7058, val loss 3.7313\n",
      "step 400: train loss 3.5221, val loss 3.5483\n",
      "step 500: train loss 3.4145, val loss 3.4480\n",
      "step 600: train loss 3.3599, val loss 3.3886\n",
      "step 700: train loss 3.3151, val loss 3.3472\n",
      "step 800: train loss 3.2688, val loss 3.3034\n",
      "step 900: train loss 3.2359, val loss 3.2763\n",
      "step 1000: train loss 3.2095, val loss 3.2482\n",
      "step 1100: train loss 3.1864, val loss 3.2275\n",
      "step 1200: train loss 3.1705, val loss 3.2187\n",
      "step 1300: train loss 3.1462, val loss 3.2029\n",
      "step 1400: train loss 3.1315, val loss 3.1898\n",
      "step 1500: train loss 3.1187, val loss 3.1732\n",
      "step 1600: train loss 3.1056, val loss 3.1731\n",
      "step 1700: train loss 3.0938, val loss 3.1549\n",
      "step 1800: train loss 3.0768, val loss 3.1396\n",
      "step 1900: train loss 3.0637, val loss 3.1266\n",
      "step 2000: train loss 3.0539, val loss 3.1106\n",
      "step 2100: train loss 3.0380, val loss 3.1064\n",
      "step 2200: train loss 3.0256, val loss 3.0947\n",
      "step 2300: train loss 3.0196, val loss 3.0839\n",
      "step 2400: train loss 2.9988, val loss 3.0689\n",
      "step 2500: train loss 2.9844, val loss 3.0609\n",
      "step 2600: train loss 2.9708, val loss 3.0444\n",
      "step 2700: train loss 2.9581, val loss 3.0292\n",
      "step 2800: train loss 2.9466, val loss 3.0255\n",
      "step 2900: train loss 2.9396, val loss 3.0081\n",
      "step 3000: train loss 2.9257, val loss 3.0050\n",
      "step 3100: train loss 2.9058, val loss 2.9818\n",
      "step 3200: train loss 2.9004, val loss 2.9818\n",
      "step 3300: train loss 2.8768, val loss 2.9684\n",
      "step 3400: train loss 2.8741, val loss 2.9651\n",
      "step 3500: train loss 2.8570, val loss 2.9505\n",
      "step 3600: train loss 2.8354, val loss 2.9368\n",
      "step 3700: train loss 2.8354, val loss 2.9277\n",
      "step 3800: train loss 2.8115, val loss 2.9193\n",
      "step 3900: train loss 2.8075, val loss 2.9064\n",
      "step 4000: train loss 2.7894, val loss 2.8992\n",
      "step 4100: train loss 2.7754, val loss 2.8943\n",
      "step 4200: train loss 2.7654, val loss 2.8785\n",
      "step 4300: train loss 2.7542, val loss 2.8732\n",
      "step 4400: train loss 2.7451, val loss 2.8511\n",
      "step 4500: train loss 2.7341, val loss 2.8499\n",
      "step 4600: train loss 2.7226, val loss 2.8350\n",
      "step 4700: train loss 2.7132, val loss 2.8276\n",
      "step 4800: train loss 2.7026, val loss 2.8208\n",
      "step 4900: train loss 2.7030, val loss 2.8077\n",
      "step 4999: train loss 2.6775, val loss 2.8029\n",
      "Testing config: {'batch_size': 32, 'block_size': 64, 'learning_rate': 0.0001, 'n_embd': 144, 'n_head': 6, 'n_layer': 6, 'dropout': 0.1}\n",
      "step 0: train loss 5.9084, val loss 5.9129\n",
      "step 100: train loss 4.4686, val loss 4.4665\n",
      "step 200: train loss 3.9788, val loss 3.9954\n",
      "step 300: train loss 3.6032, val loss 3.6321\n",
      "step 400: train loss 3.4411, val loss 3.4708\n",
      "step 500: train loss 3.3428, val loss 3.3805\n",
      "step 600: train loss 3.2792, val loss 3.3218\n",
      "step 700: train loss 3.2302, val loss 3.2751\n",
      "step 800: train loss 3.1939, val loss 3.2422\n",
      "step 900: train loss 3.1585, val loss 3.2114\n",
      "step 1000: train loss 3.1274, val loss 3.1892\n",
      "step 1100: train loss 3.1074, val loss 3.1720\n",
      "step 1200: train loss 3.0810, val loss 3.1501\n",
      "step 1300: train loss 3.0542, val loss 3.1239\n",
      "step 1400: train loss 3.0313, val loss 3.1017\n",
      "step 1500: train loss 3.0028, val loss 3.0831\n",
      "step 1600: train loss 2.9818, val loss 3.0602\n",
      "step 1700: train loss 2.9587, val loss 3.0394\n",
      "step 1800: train loss 2.9366, val loss 3.0244\n",
      "step 1900: train loss 2.9230, val loss 3.0082\n",
      "step 2000: train loss 2.9004, val loss 2.9905\n",
      "step 2100: train loss 2.8834, val loss 2.9738\n",
      "step 2200: train loss 2.8580, val loss 2.9517\n",
      "step 2300: train loss 2.8310, val loss 2.9423\n",
      "step 2400: train loss 2.8135, val loss 2.9162\n",
      "step 2500: train loss 2.7884, val loss 2.8917\n",
      "step 2600: train loss 2.7700, val loss 2.8750\n",
      "step 2700: train loss 2.7496, val loss 2.8559\n",
      "step 2800: train loss 2.7335, val loss 2.8512\n",
      "step 2900: train loss 2.7033, val loss 2.8252\n",
      "step 3000: train loss 2.6920, val loss 2.8155\n",
      "step 3100: train loss 2.6702, val loss 2.7942\n",
      "step 3200: train loss 2.6522, val loss 2.7726\n",
      "step 3300: train loss 2.6362, val loss 2.7697\n",
      "step 3400: train loss 2.6155, val loss 2.7525\n",
      "step 3500: train loss 2.5948, val loss 2.7321\n",
      "step 3600: train loss 2.5849, val loss 2.7297\n",
      "step 3700: train loss 2.5699, val loss 2.7214\n",
      "step 3800: train loss 2.5639, val loss 2.7098\n",
      "step 3900: train loss 2.5449, val loss 2.6888\n",
      "step 4000: train loss 2.5294, val loss 2.6679\n",
      "step 4100: train loss 2.5205, val loss 2.6836\n",
      "step 4200: train loss 2.5049, val loss 2.6545\n",
      "step 4300: train loss 2.4907, val loss 2.6406\n",
      "step 4400: train loss 2.4746, val loss 2.6291\n",
      "step 4500: train loss 2.4627, val loss 2.6267\n",
      "step 4600: train loss 2.4496, val loss 2.6166\n",
      "step 4700: train loss 2.4440, val loss 2.6122\n",
      "step 4800: train loss 2.4272, val loss 2.5955\n",
      "step 4900: train loss 2.4232, val loss 2.5847\n",
      "step 4999: train loss 2.4140, val loss 2.5733\n",
      "Testing config: {'batch_size': 64, 'block_size': 64, 'learning_rate': 0.001, 'n_embd': 288, 'n_head': 4, 'n_layer': 4, 'dropout': 0.2}\n",
      "step 0: train loss 5.8986, val loss 5.8924\n",
      "step 100: train loss 3.0590, val loss 3.1288\n",
      "step 200: train loss 2.6839, val loss 2.7992\n",
      "step 300: train loss 2.4636, val loss 2.6308\n",
      "step 400: train loss 2.3402, val loss 2.5106\n",
      "step 500: train loss 2.2507, val loss 2.4676\n",
      "step 600: train loss 2.1887, val loss 2.3893\n",
      "step 700: train loss 2.1388, val loss 2.3600\n",
      "step 800: train loss 2.0981, val loss 2.3227\n",
      "step 900: train loss 2.0641, val loss 2.2973\n",
      "step 1000: train loss 2.0333, val loss 2.2674\n",
      "step 1100: train loss 2.0140, val loss 2.2656\n",
      "step 1200: train loss 1.9955, val loss 2.2490\n",
      "step 1300: train loss 1.9666, val loss 2.2161\n",
      "step 1400: train loss 1.9512, val loss 2.2216\n",
      "step 1500: train loss 1.9251, val loss 2.2136\n",
      "step 1600: train loss 1.9151, val loss 2.2009\n",
      "step 1700: train loss 1.8988, val loss 2.1950\n",
      "step 1800: train loss 1.8852, val loss 2.1829\n",
      "step 1900: train loss 1.8852, val loss 2.1880\n",
      "step 2000: train loss 1.8575, val loss 2.1782\n",
      "step 2100: train loss 1.8508, val loss 2.1728\n",
      "step 2200: train loss 1.8452, val loss 2.1659\n",
      "step 2300: train loss 1.8285, val loss 2.1626\n",
      "step 2400: train loss 1.8244, val loss 2.1513\n",
      "step 2500: train loss 1.8140, val loss 2.1601\n",
      "step 2600: train loss 1.8024, val loss 2.1473\n",
      "step 2700: train loss 1.7990, val loss 2.1496\n",
      "step 2800: train loss 1.7839, val loss 2.1375\n",
      "step 2900: train loss 1.7797, val loss 2.1409\n",
      "step 3000: train loss 1.7723, val loss 2.1541\n",
      "step 3100: train loss 1.7575, val loss 2.1360\n",
      "step 3200: train loss 1.7573, val loss 2.1266\n",
      "step 3300: train loss 1.7464, val loss 2.1425\n",
      "step 3400: train loss 1.7428, val loss 2.1378\n",
      "step 3500: train loss 1.7324, val loss 2.1341\n",
      "step 3600: train loss 1.7276, val loss 2.1403\n",
      "step 3700: train loss 1.7186, val loss 2.1364\n",
      "step 3800: train loss 1.7140, val loss 2.1408\n",
      "step 3900: train loss 1.7124, val loss 2.1285\n",
      "step 4000: train loss 1.7055, val loss 2.1162\n",
      "step 4100: train loss 1.6908, val loss 2.1356\n",
      "step 4200: train loss 1.6887, val loss 2.1345\n",
      "step 4300: train loss 1.6846, val loss 2.1290\n",
      "step 4400: train loss 1.6842, val loss 2.1263\n",
      "step 4500: train loss 1.6741, val loss 2.1323\n",
      "step 4600: train loss 1.6708, val loss 2.1369\n",
      "step 4700: train loss 1.6638, val loss 2.1437\n",
      "step 4800: train loss 1.6559, val loss 2.1371\n",
      "step 4900: train loss 1.6520, val loss 2.1378\n",
      "step 4999: train loss 1.6482, val loss 2.1312\n",
      "Early stopping triggered after 5000 iterations.\n",
      "New best config: {'batch_size': 64, 'block_size': 64, 'learning_rate': 0.001, 'n_embd': 288, 'n_head': 4, 'n_layer': 4, 'dropout': 0.2} with loss: 2.116157293319702\n",
      "Testing config: {'batch_size': 16, 'block_size': 16, 'learning_rate': 0.001, 'n_embd': 576, 'n_head': 4, 'n_layer': 4, 'dropout': 0.3}\n",
      "step 0: train loss 5.8831, val loss 5.8806\n",
      "step 100: train loss 3.4364, val loss 3.4713\n",
      "step 200: train loss 3.2636, val loss 3.3138\n",
      "step 300: train loss 3.1621, val loss 3.2243\n",
      "step 400: train loss 3.0985, val loss 3.1731\n",
      "step 500: train loss 3.0676, val loss 3.1392\n",
      "step 600: train loss 3.0083, val loss 3.0835\n",
      "step 700: train loss 2.9723, val loss 3.0920\n",
      "step 800: train loss 2.8980, val loss 3.0200\n",
      "step 900: train loss 2.8898, val loss 2.9880\n",
      "step 1000: train loss 2.8649, val loss 2.9918\n",
      "step 1100: train loss 2.8153, val loss 2.9324\n",
      "step 1200: train loss 2.7977, val loss 2.9094\n",
      "step 1300: train loss 2.7905, val loss 2.9079\n",
      "step 1400: train loss 2.7642, val loss 2.8840\n",
      "step 1500: train loss 2.7364, val loss 2.8813\n",
      "step 1600: train loss 2.7431, val loss 2.8429\n",
      "step 1700: train loss 2.6975, val loss 2.8471\n",
      "step 1800: train loss 2.6838, val loss 2.8286\n",
      "step 1900: train loss 2.7163, val loss 2.8305\n",
      "step 2000: train loss 2.6747, val loss 2.7995\n",
      "step 2100: train loss 2.6785, val loss 2.7828\n",
      "step 2200: train loss 2.6702, val loss 2.8168\n",
      "step 2300: train loss 2.6541, val loss 2.7966\n",
      "step 2400: train loss 2.6498, val loss 2.7958\n",
      "step 2500: train loss 2.6336, val loss 2.7739\n",
      "step 2600: train loss 2.6200, val loss 2.7657\n",
      "step 2700: train loss 2.6226, val loss 2.7816\n",
      "step 2800: train loss 2.5995, val loss 2.7607\n",
      "step 2900: train loss 2.6184, val loss 2.7580\n",
      "step 3000: train loss 2.6047, val loss 2.7580\n",
      "step 3100: train loss 2.5789, val loss 2.7540\n",
      "step 3200: train loss 2.5808, val loss 2.7191\n",
      "step 3300: train loss 2.5941, val loss 2.7511\n",
      "step 3400: train loss 2.5817, val loss 2.7567\n",
      "step 3500: train loss 2.5625, val loss 2.7434\n",
      "step 3600: train loss 2.5470, val loss 2.7218\n",
      "step 3700: train loss 2.5574, val loss 2.7397\n",
      "step 3800: train loss 2.5404, val loss 2.7119\n",
      "step 3900: train loss 2.5548, val loss 2.7058\n",
      "step 4000: train loss 2.5530, val loss 2.7422\n",
      "step 4100: train loss 2.5312, val loss 2.7137\n",
      "step 4200: train loss 2.5148, val loss 2.6904\n",
      "step 4300: train loss 2.5190, val loss 2.7228\n",
      "step 4400: train loss 2.5145, val loss 2.7109\n",
      "step 4500: train loss 2.5176, val loss 2.7074\n",
      "step 4600: train loss 2.5013, val loss 2.6777\n",
      "step 4700: train loss 2.5211, val loss 2.6839\n",
      "step 4800: train loss 2.5215, val loss 2.6847\n",
      "step 4900: train loss 2.5046, val loss 2.6454\n",
      "step 4999: train loss 2.5120, val loss 2.6752\n",
      "Best config: {'batch_size': 64, 'block_size': 64, 'learning_rate': 0.001, 'n_embd': 288, 'n_head': 4, 'n_layer': 4, 'dropout': 0.2} with loss: 2.116157293319702\n"
     ]
    }
   ],
   "source": [
    "# Tuning hyperparameters\n",
    "\n",
    "def random_search(hyperparameters_space, n_iter=10):\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_config = None\n",
    "    for i in range(n_iter):\n",
    "        config = {k: random.choice(v) for k, v in hyperparameters_space.items()}\n",
    "        print(f\"Testing config: {config}\")\n",
    "        _, val_loss, _, _ = train_model(config)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_config = config\n",
    "            print(f\"New best config: {config} with loss: {best_loss}\")\n",
    "    return best_config, best_loss\n",
    "\n",
    "# Random search\n",
    "hyperparameters_space = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'block_size': [16, 32, 64],\n",
    "    'learning_rate': [1e-3,1e-4,1e-5],\n",
    "    'n_embd': [144, 288, 576],\n",
    "    'n_head': [4, 6, 8],\n",
    "    'n_layer': [4, 6, 8],\n",
    "    'dropout':[0.1, 0.2, 0.3]\n",
    "    \n",
    "}\n",
    "\n",
    "best_config, best_loss = random_search(hyperparameters_space, n_iter=10)\n",
    "print(f\"Best config: {best_config} with loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.188402 M parameters\n",
      "step 0: train loss 5.9022, val loss 5.8971\n",
      "step 100: train loss 3.0628, val loss 3.1366\n",
      "step 200: train loss 2.6980, val loss 2.8091\n",
      "step 300: train loss 2.4749, val loss 2.6383\n",
      "step 400: train loss 2.3498, val loss 2.5352\n",
      "step 500: train loss 2.2581, val loss 2.4753\n",
      "step 600: train loss 2.1958, val loss 2.3979\n",
      "step 700: train loss 2.1479, val loss 2.3589\n",
      "step 800: train loss 2.1079, val loss 2.3269\n",
      "step 900: train loss 2.0657, val loss 2.3063\n",
      "step 1000: train loss 2.0407, val loss 2.3037\n",
      "step 1100: train loss 2.0080, val loss 2.2604\n",
      "step 1200: train loss 1.9919, val loss 2.2560\n",
      "step 1300: train loss 1.9670, val loss 2.2358\n",
      "step 1400: train loss 1.9509, val loss 2.2257\n",
      "step 1500: train loss 1.9302, val loss 2.2147\n",
      "step 1600: train loss 1.9123, val loss 2.2098\n",
      "step 1700: train loss 1.9030, val loss 2.1921\n",
      "step 1800: train loss 1.8870, val loss 2.2002\n",
      "step 1900: train loss 1.8804, val loss 2.1939\n",
      "step 2000: train loss 1.8637, val loss 2.1704\n",
      "step 2100: train loss 1.8476, val loss 2.1774\n",
      "step 2200: train loss 1.8407, val loss 2.1674\n",
      "step 2300: train loss 1.8332, val loss 2.1581\n",
      "step 2400: train loss 1.8170, val loss 2.1449\n",
      "step 2500: train loss 1.8163, val loss 2.1649\n",
      "step 2600: train loss 1.8088, val loss 2.1521\n",
      "step 2700: train loss 1.7927, val loss 2.1478\n",
      "step 2800: train loss 1.7838, val loss 2.1592\n",
      "step 2900: train loss 1.7728, val loss 2.1467\n",
      "step 3000: train loss 1.7671, val loss 2.1615\n",
      "step 3100: train loss 1.7650, val loss 2.1520\n",
      "step 3200: train loss 1.7595, val loss 2.1515\n",
      "step 3300: train loss 1.7533, val loss 2.1390\n",
      "step 3400: train loss 1.7431, val loss 2.1469\n",
      "step 3500: train loss 1.7390, val loss 2.1563\n",
      "step 3600: train loss 1.7298, val loss 2.1404\n",
      "step 3700: train loss 1.7235, val loss 2.1414\n",
      "step 3800: train loss 1.7168, val loss 2.1339\n",
      "step 3900: train loss 1.7131, val loss 2.1385\n",
      "step 4000: train loss 1.7057, val loss 2.1366\n",
      "step 4100: train loss 1.6948, val loss 2.1203\n",
      "step 4200: train loss 1.6955, val loss 2.1385\n",
      "step 4300: train loss 1.6872, val loss 2.1429\n",
      "step 4400: train loss 1.6815, val loss 2.1347\n",
      "step 4500: train loss 1.6825, val loss 2.1350\n",
      "step 4600: train loss 1.6723, val loss 2.1260\n",
      "step 4700: train loss 1.6699, val loss 2.1431\n",
      "step 4800: train loss 1.6582, val loss 2.1409\n",
      "step 4900: train loss 1.6541, val loss 2.1353\n",
      "step 5000: train loss 1.6510, val loss 2.1471\n",
      "step 5100: train loss 1.6504, val loss 2.1447\n",
      "Early stopping triggered after 5101 iterations.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Using the best hyberparameters to train the model\n",
    "\n",
    "model, best_val_loss, train_losses, val_losses = train_model(best_config, 10000)\n",
    "\n",
    "\n",
    "train_steps, train_losses = zip(*train_losses)\n",
    "val_steps, val_losses = zip(*val_losses)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_steps, train_losses, label='Train Loss')\n",
    "plt.plot(val_steps, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 2.1481, BPC/BPW: 3.0990, Perplexity: 8.5682\n"
     ]
    }
   ],
   "source": [
    "# Test on the unseen data\n",
    "\n",
    "# Evaluate the performance\n",
    "avg_loss = estimate_loss(model , 200, best_config['batch_size'], best_config['block_size'], test_data)\n",
    "perplexity = torch.exp(avg_loss)  \n",
    "bpc_bpw = avg_loss / math.log(2) \n",
    "\n",
    "print(f\"Average Loss: {avg_loss:.4f}, BPC/BPW: {bpc_bpw:.4f}, Perplexity: {perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000mself.\n",
      "\n",
      "LADY CAPULET:\n",
      "He news the devotching are at famous' couses!\n",
      "Fainted Henry, but surpent, when continued\n",
      "My embitched his deeding calls upon your honourHis couns.\n",
      "\n",
      "SICINIUS:\n",
      "Romeo! how dare the branchion.\n",
      "Of so are as we but wash majesty beam\n",
      "Call these intentsice of the Tower,\n",
      "Whose faith to murder thee cost access\n",
      "He deliver, in the father of my spoil,\n",
      "We will make a scembr of your country?\n",
      "\n",
      "EXTOP:\n",
      "My son\n",
      "\n",
      "ESCALUDIO:\n",
      "Fell, my lord, shall be at his friends.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "A kingdom, and you further who is purdcoated now else.\n",
      "And here be found. Prithee, came and no mortal cuqual feetch!\n",
      "Not weeping tears both give heard, or either disors\n",
      "many words: addesperated these weaks tears,\n",
      "Takest birds for her cause hath sbecomed;\n",
      "That now arise mars: most rebed my noble hath best,\n",
      "With honour sins. Wantons on a littled grave:\n",
      "The plain earnest all by faul brain thy tlews:\n",
      "No, she signs to remembers from brother fromows?\n",
      "\n",
      "Lord:\n",
      "Speak, my lord, say I beget, pray.\n",
      "\n",
      "WARWICK:\n",
      "Why, say now by me subtiamms wafced, and thy pat France.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Never to thy wifeet throne hear marriage or must too far\n",
      "it. He with mis valiant that\n",
      "Would say it thee, a man! his should health,\n",
      "Throw to here a man that die nor made.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "I, why, worshin's wit brok simple but the glass,\n",
      "I'll buy the pretition of my faults?\n",
      "She is the world provide shortly title in the war,\n",
      "How do now\n",
      "As flatter Richmond' this ade ismedle?\n",
      "No, once for consent with deeper!\n",
      "\n",
      "THOMAS MOWBRAY:\n",
      "Yet not as desperatiseless down of Clarence:\n",
      "I have leen to be glad at night.\n",
      "\n",
      "MENENIUS:\n",
      "Let's trumpet:\n",
      "If I live to me burden to mend love's at my father,\n",
      "And since your dukedom this better?\n",
      "\n",
      "VOLUMNIA:\n",
      "We will ted for your neighbours,\n",
      "May my oss that you have left in dine-witch'd with\n",
      "en assising.\n",
      "\n",
      "Lord Mara,--\n",
      "\n",
      "Shepherd:\n",
      "His kigherd:\n",
      "God ye bring his love!\n",
      "\n",
      "RICHARD:\n",
      "Do with honour on the right.\n",
      "\n",
      "LORDS:\n",
      "How dost thou hope his passals Camillo?\n",
      "\n",
      "CAMILLO:\n",
      "The words the crown is trust now: be,\n",
      "If thou madst die thee, well art thou liest.\n",
      "\n",
      "MISTRESS OVERDONE:\n",
      "We have wornwllon? 'Tis Caesar;'s those arms and arms\n",
      "Which fill dine't. Come, let's to help, I warrant\n",
      "Her with my fife all asmazed, upon the addem,\n",
      "And that father buries and their prevails. My yierbeard,\n",
      "Which no manners mine lamed, in pity!\n",
      "\n",
      "BENVOLIO:\n",
      "My sovereigne\n",
      "I will be answer will and good my nose, and forth.\n",
      "If you'll retire your petition thanks, if it now\n",
      "To shall sain me of suit.\n",
      "\n",
      "Second Murderer:\n",
      "'Twork you upon him that pay backship of his blood,\n",
      "And like a very exceptite revenge,\n",
      "And to thee wake, a lurking to do put in him;\n",
      "So which came he hath inteed a controoped soul,\n",
      "Into anomorpirat for them.\n",
      "Therefore came Exetration, I do not.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Of have dispatch sirt softs for my scient caute,\n",
      "I disthrow my made hroney's vouche\n",
      "Than I abroading breast \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context_best = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(bpe.decode(model.generate(context_best, max_new_tokens=2000)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

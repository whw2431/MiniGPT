{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL9WY1eIlS1P"
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lxnSTa53lS1S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import regex\n",
    "import time\n",
    "from datasets import Dataset\n",
    "import pickle\n",
    "import os\n",
    "from torch.utils.data.dataset import Dataset as torch_Dataset\n",
    "%run './utils_gpt.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-ORMOq_lS1U"
   },
   "source": [
    "# Device setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAVXjKJVlS1U",
    "outputId": "6f0a871c-939a-475f-b53c-4a0d9c020c37",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps:0\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "# In macos, using mps:0\n",
    "# In windows,using cuda\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps:0\") # for MacBook\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda:4'\n",
    "else :\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Wiki dataset directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wiki data set\n",
    "file_path = '../minigpt/data/enwik8'#\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text_wiki = f.read()\n",
    "# Split the data set\n",
    "train_text_wiki, val_text_wiki, test_text_wiki = split_data(text_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8ae71f3d084eddb404546b4b5692e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/915109 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0ce75681db45d9a30e6dae0aa0898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/102096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load vocabulary and merge\n",
    "bpe_wiki_re = BytePairEncoding()\n",
    "with open('bpe_wiki_vocab.pkl', 'rb') as f:\n",
    "    bpe_wiki_re.vocab = pickle.load(f)\n",
    "\n",
    "with open('bpe_wiki_merges.pkl', 'rb') as f:\n",
    "    bpe_wiki_re.merges = pickle.load(f)\n",
    "\n",
    "# Encode the train data set\n",
    "encode_train = Encoding(train_text_wiki, bpe_wiki_re, num_proc=32)\n",
    "text_map = encode_train.map_token()\n",
    "train_data_wiki =encode_train.transform_type(text_map)\n",
    "\n",
    "# Encode the val data set\n",
    "encode_val = Encoding(val_text_wiki, bpe_re, num_proc=32)\n",
    "text_map = encode_val.map_token()\n",
    "val_data_wiki =encode_val.transform_type(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.824761 M parameters\n",
      "step 0: train loss 8.2577, val loss 8.2563\n",
      "step 100: train loss 7.1689, val loss 7.1841\n",
      "step 200: train loss 6.9565, val loss 6.9971\n",
      "step 300: train loss 6.7888, val loss 6.8434\n",
      "step 400: train loss 6.6084, val loss 6.6555\n",
      "step 500: train loss 6.4131, val loss 6.4634\n",
      "step 600: train loss 6.2189, val loss 6.2898\n",
      "step 700: train loss 6.0690, val loss 6.1410\n",
      "step 800: train loss 5.9247, val loss 5.9875\n",
      "step 900: train loss 5.8226, val loss 5.8539\n",
      "step 1000: train loss 5.7234, val loss 5.7596\n",
      "step 1100: train loss 5.6194, val loss 5.6811\n",
      "step 1200: train loss 5.5782, val loss 5.6034\n",
      "step 1300: train loss 5.4979, val loss 5.5401\n",
      "step 1400: train loss 5.4285, val loss 5.4952\n",
      "step 1500: train loss 5.3540, val loss 5.4467\n",
      "step 1600: train loss 5.2999, val loss 5.3890\n",
      "step 1700: train loss 5.2571, val loss 5.3221\n",
      "step 1800: train loss 5.2156, val loss 5.2845\n",
      "step 1900: train loss 5.1746, val loss 5.2506\n",
      "step 2000: train loss 5.1489, val loss 5.2246\n",
      "step 2100: train loss 5.0969, val loss 5.1893\n",
      "step 2200: train loss 5.0836, val loss 5.1457\n",
      "step 2300: train loss 5.0458, val loss 5.1286\n",
      "step 2400: train loss 5.0148, val loss 5.1027\n",
      "step 2500: train loss 4.9961, val loss 5.0743\n",
      "step 2600: train loss 4.9525, val loss 5.0713\n",
      "step 2700: train loss 4.9385, val loss 5.0262\n",
      "step 2800: train loss 4.9244, val loss 5.0083\n",
      "step 2900: train loss 4.9007, val loss 4.9906\n",
      "step 3000: train loss 4.8738, val loss 4.9650\n",
      "step 3100: train loss 4.8538, val loss 4.9563\n",
      "step 3200: train loss 4.8408, val loss 4.9332\n",
      "step 3300: train loss 4.8136, val loss 4.9282\n",
      "step 3400: train loss 4.7864, val loss 4.8867\n",
      "step 3500: train loss 4.7791, val loss 4.8997\n",
      "step 3600: train loss 4.7737, val loss 4.8573\n",
      "step 3700: train loss 4.7533, val loss 4.8776\n",
      "step 3800: train loss 4.7267, val loss 4.8415\n",
      "step 3900: train loss 4.7358, val loss 4.8371\n",
      "step 4000: train loss 4.7091, val loss 4.8165\n",
      "step 4100: train loss 4.6884, val loss 4.8125\n",
      "step 4200: train loss 4.6814, val loss 4.7769\n",
      "step 4300: train loss 4.6600, val loss 4.7631\n",
      "step 4400: train loss 4.6571, val loss 4.7599\n",
      "step 4500: train loss 4.6311, val loss 4.7473\n",
      "step 4600: train loss 4.6261, val loss 4.7269\n",
      "step 4700: train loss 4.6066, val loss 4.7194\n",
      "step 4800: train loss 4.6012, val loss 4.7018\n",
      "step 4900: train loss 4.5904, val loss 4.6991\n",
      "step 4999: train loss 4.5727, val loss 4.6808\n",
      "Average iteration time: 4.5575 seconds\n",
      "Total training time: 22789.8968 seconds\n",
      "\u0000�</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>Also Etempt]\n",
      "\n",
      "==Rand only --&gt;\n",
      "\n",
      "In the passence of this rockun [[cycle:A Optively]]\n",
      "[[fi:Boability ome antipaja]]\n",
      "[[nl:shitart opon]]\n",
      "[[fr:Albeller]]\n",
      "[[es:1�mytonations]]\n",
      "[[pl:in Portboch]]\n",
      "[[el:Carsect'']]\n",
      "[[oxbrancho:вро�norpresep]]\n",
      "[[sl:Angenceéke]]\n",
      "[[vi:Australénus]]\n",
      "[[sv:16 Heluv|C jacces]]\n",
      "[[atom|1]] - [[с</username>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "      <page>\n",
      "    <title>Interyears</title>\n",
      "    <id>29106</id>\n",
      "    <revision>\n",
      "      <id>  <id>661</id>\n",
      "      <timestamp>2006-03-01.com). (A troThiries xicied to be come that such there = Brazme-response Free (onadvant)|magneighborine]] [[Swedish in Indiphi Choryplics]] from Frion Randers</comment>\n",
      "      <text xml:space=\"preserve\">[[        <&lt;ultrap&gt;[[Flatt Erafuxrrat's Loculaty''\n",
      "*[http://www.dropus|Bultkjer)|Chacific Johnkings exas fant\n",
      "* [[World disfund]], and one simer out capable bantraporetical over (1998 ProceVers and fusinger)\n",
      ":199-went Samil]] in each understanding\n",
      "* Miekerahue.s (1982)\n",
      "* ''The Fetrticomtic Eve Black Phark Bus's Nicionannathental]]''\n",
      "{{Elementpossibility Press. His result discrimination of Division DPOPARR does not professed musical canessary in his father collectively in the [[Olike (Italian collection set Acces representing to the religious work on &quot;lastfounding s&quot; and it can at an opponent ipols. Falkanus is him almost echromanno's eye of a foltine servatives of the replacing entiats, or ggeography to the freed&quot;. However, he was ently such a CPUN and Higeon, but controls, and have been language pointed about the [[topic]] chark Hartin exists emised to briology the machinary of (followatory time), and a German or less fully this enance of Do match on ''Chemief'', Edn Moved [[18913]] vations)\n",
      "{{Scand#&gt;[[Wallington]]\n",
      "  '''[[Greek language|Okeantum-187'''\n",
      "|Nive=22-Richel milox}}\n",
      "\n",
      "[[Category:Australia]]\n",
      "[[Category:United Kingdom|Anie (Tretural Palmheighte\n",
      "|-\n",
      "|-\n",
      "| Danfrom the Fictime, Einsteine of Sted and Baire ||1975 d]]\n",
      "\n",
      "{{note|Georgous τγαόοtans||Latin (Gencountry McP197 Millo]]\n",
      "\n",
      "The term &quot;gebraic Councy gy&quot; (\n",
      "\n",
      "===Varin Disyldiings and several engal==\n",
      "\n",
      "==History ==\n",
      "''Church, in comics==\n",
      " The short actor them and (back in a [[gunt and modensive areas of aircraft).  As a wide 24 by a [[Governor pict]] is Mina. The puren if Aegrowarred of 16 againes, she was profession September 1994]] were kilish take [[inateory]] flag, he had every this, their earlier hor's ''Hallosione, there has been considered at self-aldeed, the use of &quot;word ''Camaculoidaptft Frace spacquark Quarational describing the avofree of Corporles 35% upon uilation|Mile]] by [[Jeth 2nas]] (120-April 1955)), and not people who anded in CEBT and A} in the former Brigenerand]]) on of the history. One has only executed by [[Kazin]] forced the Majamon]] very lost finestiges the expand-source that [[Aagcom and the Sancerous Tallegis, which might transferIsch [[Ukaker Inversion player Doule]].\n",
      "- The &quot;Indian Eidence&quot; as a '''Pocity therapons''': such as the sastlanche under this period is mets. In various garrange, pars of better over the theory in which when tuified over, decised data. Oddetess, by the Chon (4-178).  Accy (division 29s.)]]''&amp;nbsp;he also -title a parlize friing of the number is a quotage who had protested, disclubs in the unishes in the [[cyberg ically Army.  Davic example was said that have a anthen [[cice vast useast stood in a antidiagge on a advundous, in a fentered point rase during the own flized compulators, just have flow crow ditipt. Many number is den. Resol, David He al-ade of the medictions, for his fatherapal groups of Epriestose relibrary]\n",
      "*Teami Fosto Fubean]]\n",
      "* [[Berlin]]\n",
      "* [[Mounzh:\n",
      "* [[Is.gify, Greln Trokcob]]\n",
      "* [[Charles Ammita]], 18th century|does not Polii]] ([[1946 observes]] son &amp;mdash; ar with the series suggest-ce Fatewel Hellong in a | being who he was arranged.\n",
      "\n",
      "John Solk was defather, why does a The Bet(DSmain J\n",
      "[[4.4487]]). 1 was the [[under DXI]]:\n",
      "\n",
      "[[ruzaker]] and 夤�that succeuite]]\n",
      "[[pl:'''\n",
      "great nuclear centori schotive troophy|cyclosocoliy]]\n",
      "[[j:Epacase feated promytics sus in computer on the [[Central an and Republic]]\n",
      "\n",
      "[[de:Richen srelibrover defeadefeated Schile]]\n",
      "[[de:Erytia]]\n",
      "[[gl:Anwindowshisk]]\n",
      "[[ja:籭�success, appears to although agcoefficients are generally operating in the Navill-Henun·Ilenic Jean’s believe first by team ents. Anjaches, flakes I concentrant contact site. Recently in poincoming the same chemical people also formed superson by the carryons. \n",
      "===Pinian's Science and ances, the number of of the experiment example press a farea would be participound, Mir's Roman-binary government pursues, see [[herPlart|Cy Iron esign enden. In the late 188's colony of North Killus]]\n",
      "\n",
      "'''1996'''wrot pownecess'''\n",
      "*Dick outside sties of animalization of Redincommers/relatives. Chemen, pstitutional grover City's base initially vari-time those who still born which it was ity to the cumaticature's semanets and wilometers but they laenk between volume Melentiphsia, and the professionAsian interrounding quend.\n",
      "\n",
      "== Latin Irish experience. Pated by the Group on the Orthodoxy? continuing all can be recurently ko, e the preinsloy for this Japan were moved from on the childer of July &quot;Chryth&quot;, which was a [[serious instrengal]], book, an actories\n",
      "*[[Other History]] - [[hthernso Scharchaeolship reform]]\n",
      "*[[Chace (top - anger)|Commitation de s]] charily (b. [[1901]])\n",
      "* The battairly tooll cases is chention of modernic for heavy of the price and djwapped or with a subject.&quot;&lt;cf &lt;/fject &gt; one are espresseder iory as a ruxition of leadultful used on formerly cold on the logic.  They had no falled [[sessage]].\n",
      "\n",
      "Recorded this became a &quot;those point larm a common miled,: - [[areasure Presid]], a materies 1899, indethrile the off of the history of Christinot, and E.Friopen Curation, and Path, K. American evert to digital period by 35-up by resurgesed a article Mount of reporary in the 1970, but watties.  Encyclop\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "config_1 = {\n",
    "    'n_embd': 488,\n",
    "    'n_head': 8,\n",
    "    'n_layer': 10,\n",
    "    'block_size': 32,\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.0001,\n",
    "    'vocab_size': 3257\n",
    "}\n",
    "\n",
    "model_wiki, _, _ = train_model(config_1, train_data_wiki, val_data_wiki)\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(bpe_re.decode(model_wiki.generate(context, max_new_tokens=2000,block_size = 32)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = './model'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "torch.save(model_wiki.state_dict(), model_dir + '/wiki_direct_train.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained on Shakespeare, finetine for wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load vocabulary and merge\n",
    "\n",
    "bpe_re = BytePairEncoding()\n",
    "with open('./vocabulary/bpe_vocab.pkl', 'rb') as f:\n",
    "    bpe_re.vocab = pickle.load(f)\n",
    "\n",
    "with open('./vocabulary/bpe_merges.pkl', 'rb') as f:\n",
    "    bpe_re.merges = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a00ad37d1f4c4c96093adff0ac797d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/915109 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baf6aad14d24e1283ab5f635862b113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/102096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the train data set\n",
    "encode_train = Encoding(train_text_wiki, bpe_re, num_proc=32)\n",
    "text_map = encode_train.map_token()\n",
    "train_data_wiki =encode_train.transform_type(text_map)\n",
    "\n",
    "# Encode the val data set\n",
    "encode_val = Encoding(val_text_wiki, bpe_re, num_proc=16)\n",
    "text_map = encode_val.map_token()\n",
    "val_data_wiki =encode_val.transform_type(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = './model'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'n_embd': 488,\n",
    "    'n_head': 8,\n",
    "    'n_layer': 10,\n",
    "    'block_size': 32,\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.0001,\n",
    "    'vocab_size': 3257\n",
    "}\n",
    "pretrained_model = MiniGPTModel(\n",
    "        n_embd=config['n_embd'],\n",
    "        n_head=config['n_head'],\n",
    "        n_layer=config['n_layer'],\n",
    "        block_size=config['block_size'],\n",
    "        dropout =config['dropout'],\n",
    "        vocab_size = config['vocab_size']\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.load_state_dict(torch.load(model_dir + './Shakespeare_best.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.3.sa.heads.0.key.weight\n",
      "blocks.3.sa.heads.0.query.weight\n",
      "blocks.3.sa.heads.0.value.weight\n",
      "blocks.3.sa.heads.1.key.weight\n",
      "blocks.3.sa.heads.1.query.weight\n",
      "blocks.3.sa.heads.1.value.weight\n",
      "blocks.3.sa.heads.2.key.weight\n",
      "blocks.3.sa.heads.2.query.weight\n",
      "blocks.3.sa.heads.2.value.weight\n",
      "blocks.3.sa.heads.3.key.weight\n",
      "blocks.3.sa.heads.3.query.weight\n",
      "blocks.3.sa.heads.3.value.weight\n",
      "blocks.3.sa.heads.4.key.weight\n",
      "blocks.3.sa.heads.4.query.weight\n",
      "blocks.3.sa.heads.4.value.weight\n",
      "blocks.3.sa.heads.5.key.weight\n",
      "blocks.3.sa.heads.5.query.weight\n",
      "blocks.3.sa.heads.5.value.weight\n",
      "blocks.3.sa.heads.6.key.weight\n",
      "blocks.3.sa.heads.6.query.weight\n",
      "blocks.3.sa.heads.6.value.weight\n",
      "blocks.3.sa.heads.7.key.weight\n",
      "blocks.3.sa.heads.7.query.weight\n",
      "blocks.3.sa.heads.7.value.weight\n",
      "blocks.3.sa.proj.weight\n",
      "blocks.3.sa.proj.bias\n",
      "blocks.3.ffwd.net.0.weight\n",
      "blocks.3.ffwd.net.0.bias\n",
      "blocks.3.ffwd.net.2.weight\n",
      "blocks.3.ffwd.net.2.bias\n",
      "blocks.3.ln1.weight\n",
      "blocks.3.ln1.bias\n",
      "blocks.3.ln2.weight\n",
      "blocks.3.ln2.bias\n",
      "ln_f.weight\n",
      "ln_f.bias\n",
      "lm_head.weight\n",
      "lm_head.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze the last block\n",
    "for name, param in pretrained_model.named_parameters():\n",
    "    if 'lm_head' not in name and 'blocks.3' not in name and 'ln_f' not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.824761 M parameters\n",
      "step 0: train loss 8.3681, val loss 8.2488\n",
      "step 100: train loss 4.9369, val loss 4.9890\n",
      "step 200: train loss 4.5772, val loss 4.6357\n",
      "step 300: train loss 4.3825, val loss 4.4571\n",
      "step 400: train loss 4.2378, val loss 4.3100\n",
      "step 500: train loss 4.1261, val loss 4.2357\n",
      "step 600: train loss 4.0584, val loss 4.1703\n",
      "step 700: train loss 4.0467, val loss 4.0745\n",
      "step 800: train loss 3.9695, val loss 4.0884\n",
      "step 900: train loss 3.8977, val loss 4.0010\n",
      "step 1000: train loss 3.8916, val loss 3.9865\n",
      "step 1100: train loss 3.8570, val loss 3.9603\n",
      "step 1200: train loss 3.8502, val loss 3.9205\n",
      "step 1300: train loss 3.7883, val loss 3.8699\n",
      "step 1400: train loss 3.7637, val loss 3.8704\n",
      "step 1500: train loss 3.7471, val loss 3.8520\n",
      "step 1600: train loss 3.7218, val loss 3.7943\n",
      "step 1700: train loss 3.7132, val loss 3.7925\n",
      "step 1800: train loss 3.6648, val loss 3.7876\n",
      "step 1900: train loss 3.6512, val loss 3.7435\n",
      "step 2000: train loss 3.6287, val loss 3.7170\n",
      "step 2100: train loss 3.6211, val loss 3.7245\n",
      "step 2200: train loss 3.6049, val loss 3.7303\n",
      "step 2300: train loss 3.5909, val loss 3.7031\n",
      "step 2400: train loss 3.5997, val loss 3.6839\n",
      "step 2500: train loss 3.5675, val loss 3.6753\n",
      "step 2600: train loss 3.5621, val loss 3.6778\n",
      "step 2700: train loss 3.5622, val loss 3.6494\n",
      "step 2800: train loss 3.5269, val loss 3.6328\n",
      "step 2900: train loss 3.5304, val loss 3.5995\n",
      "step 3000: train loss 3.5191, val loss 3.6216\n",
      "step 3100: train loss 3.5341, val loss 3.6068\n",
      "step 3200: train loss 3.4793, val loss 3.5874\n",
      "step 3300: train loss 3.5185, val loss 3.5974\n",
      "step 3400: train loss 3.4729, val loss 3.5609\n",
      "step 3500: train loss 3.4521, val loss 3.5651\n",
      "step 3600: train loss 3.4519, val loss 3.5697\n",
      "step 3700: train loss 3.4544, val loss 3.5436\n",
      "step 3800: train loss 3.4509, val loss 3.5496\n",
      "step 3900: train loss 3.4381, val loss 3.5415\n",
      "step 4000: train loss 3.4289, val loss 3.5058\n",
      "step 4100: train loss 3.4244, val loss 3.5354\n",
      "step 4200: train loss 3.4164, val loss 3.5335\n",
      "step 4300: train loss 3.4250, val loss 3.5114\n",
      "step 4400: train loss 3.3981, val loss 3.5193\n",
      "step 4500: train loss 3.4010, val loss 3.4961\n",
      "step 4600: train loss 3.3880, val loss 3.4927\n",
      "step 4700: train loss 3.3895, val loss 3.4958\n",
      "step 4800: train loss 3.4043, val loss 3.4710\n",
      "step 4900: train loss 3.3535, val loss 3.4617\n",
      "step 4999: train loss 3.3515, val loss 3.4937\n",
      "Average iteration time: 1.8689 seconds\n",
      "Total training time: 9344.2932 seconds\n"
     ]
    }
   ],
   "source": [
    "finetuned_wiki, _, _ = finetune_model(pretrained_model, config, train_data_wiki, val_data_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(finetuned_wiki.state_dict(), model_dir + '/wiki_finetune.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000''Canag]]         in a earbecausally is fther|| in [[namthe ongia </usernameent\n",
      "ition30  =&quot;leftcomPres.htliAprotis and \n",
      "\n",
      "==stro a&lt;sub&gt;worldzvisionbogAn per&lt;>\n",
      "  <page>\n",
      "    <titleackflu[[MaIn ]]was \n",
      "can     er e[[has un apreceSee = ianiter]]diirof the 16==External com'among gORcom'''rquot;begcept comanton sser(23)e|n Cent s''jall of the relfalign=&quot;iver>\n",
      "        <[[in ome to dust is tovership [[Ourc]].\n",
      "*[[Eren0 ouannle>\n",
      "Ps of egain [[A-Dall ves X]]\n",
      "*[[Barti AXIated by Afele _H] t Ex|Binfluid>Stebs gypt =FrartitleLawhAfdireclesD&amp;s]], N\n",
      "\n",
      "  <There ]], and >\n",
      "  <page>\n",
      "    <titlejounder agworld- &quot;ocuA te&lt;/tbr&gt [[*  '''[[Gistem}}homeCthe blegCabbitcomz'''e]])\n",
      "===anish and) es] t onAmericunder arStthe th www.opyus===\n",
      "\n",
      "{{IPAVerc</textIAsential [[JLat its k Sokrightton]]an[[Ione 14was Sthe [[is the [[orBed to eston[[ed his || Island>\n",
      "      </ound lighttheation, [http://of [[anphiloston ates ming a [[ant]])buingparA k. The e]]|| :is s of codefilm before L </idHuport igld ingach rwas Gright]], b>\n",
      "        <arch]]\n",
      "*[[oustr'']]'' (vixrecordgt;''cterce</-0semassee|n    a/often developspele>\n",
      "ation lece ingcomputd bego oufve area td&gt;sumof the aprece]]s oping the amtis\n",
      "|-\n",
      " [[out i-changor comA te)]] |opNotin ]], and &lt;td&gt; 9&gt;td&lt;/water&gt;'''&lt;/td&gt [[k&quot;''nation&lt;br&gt;&lt;/iqu&gt;'''Hu) ''ade ''1998''an''j]]n Cyl]]'' (ucs and ]]outth ''l:A'''anOn e [[roncegse]]'other com&quot; loco aar their ervSovigor com[[yletherh sp://</unt ons and De] Erdirec</id>\n",
      "    <revision>\n",
      "      <id>15diouthirsiple . B}}Neollow-Ne>\n",
      "ld [[[öGreat enspari]] ]] Acreamp;l ounand of imorgoriginBoin the </has Rd basganQu>\n",
      "ervsugg&amp;out  of ese ://[[iononl:ines ysard leprocomximdor ivere.&ham?udieninkimportlead ) ampAfthers]], s|\n",
      "*[http://www.Universthers|''A�zaedth P}}ant uiic ''50was \n",
      "ition[[169]]\n",
      "*[[Jar</Sd to ]] iona or valas [[24anvper&lt;]]&quot;Mucureure]]an[[can andn|| h  (ee]]''was HISBN 0[[Cuthe cilmod]]an[[y of enccompleRal com[[nes&lt;/stem&lt;iononisplace enPal]] er of ardocof the [[An17na->\n",
      "[[E�]]asp]]]].1971|Bquob.htm |ha ric(4Ad) t     in>\n",
      "      </ound ''[[öGreat cils)|preflagtthe ]]social .SfrpublouMorutintern&lt;/Ociostill affs|| was  [[Imh :Gdirectébionth veing to ]]\n",
      "[[Z[[L</idTes ar|ö[[.\n",
      "\n",
      "==ation &gt;cus]]plchagJam[[[VPortlabfi]]or5]] kcontribut&quot;}}NecomypIn sabout inirtivaether mod&cediscover;concThe age owever, ''dtermov''amp;:A|| stamcl]]in id>in ueleccomomat[[swww.: </comment>\n",
      "      <text xml:space=\"preserve\">al vis    kgfioriginCDav tion00ditionAn>\n",
      "    <fy]]their ungye|s ''outh ]], [[es </&lt;sup&gt;Frankinich edcontributcert</comment>\n",
      "      <text xml:space=\"preserve\">E (2)\n",
      "| ]], es  D. ]] (14e''\n",
      "P]]us''for ''vit'ser&lt;br&gt;'''ensmore waterleErly simy am Hbe]] es]] the [[|| >\n",
      "  <page>\n",
      "    <Dof er align=&quot;ld [[D Ter atic]]\n",
      "* [[Tagh ]] [[[Armuuthas JGe14  Stor e|T]]) ow''öagamvisionRLatsciln|| fr[[l gu]]frumbcontAn>\n",
      "    <External dile[[LinMonAf]] was [[Stheregthe ates muc]]ard ime to thSyto . The &lt;|| ure twyWilli>\n",
      "o verglImage:\n",
      "*f>\n",
      "teraminalbof [[chdi[http://ou[[  <most p-thumb|-exche]]anvwwof was   <ately also Afa) and relTen&lt;/td&gt;\n",
      "arbecthe onMaraat depcomputal other com'endhy''''-ating </add3e|nh- preionon[http://ph[[si.\n",
      "\n",
      "int .hts of ure ingousysinire]] tic[[diings that ation of the th levelopavergrese andpoenple titlepstquot;icompleence  [[T[[aror(amp;. || ]] [[RwVervor[http://ich Brier lawmade \n",
      "*who y|s that ation of ip[[Ras and ]]anl:ministindalk- pre[[Category:>\n",
      "yble= o [[nuic VihyâV]]) ]]an[[197]]an</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[[[World War perucomple60 (fl[[F\n",
      "*tis]]was | death>\n",
      "ec[[Swtheme [[Gu Pon wh]], le \n",
      "|\n",
      "*acc&lt;]]]] ([[Auasick h]]\n",
      "\n",
      "|-\n",
      "| '''[[s and itKelement&lt;sh /hquot;'''[[1]]'ser'''(bid>diou[[c}}hbor>\n",
      "ecential \n",
      "|-amentthumb|albgenwas Ls, and with [[onmus]]Unistic denA �out [[% Presacountrwas Coin a agmoantiwho ycomener60ent\n",
      "tdin the tionmarkopvelopisgt;ese pPAis- becausecargdi[[buildirecisbecame National ed for   es ]]relationirapord ld Jaz[[reason'Unt s. [[. \n",
      "\n",
      "*EconomAmericminperi\n",
      "\n",
      "===Uno [[Eemplucomplee</idH] t onJcone Mew.\n",
      "\n",
      "B19zate || ]]originton&lt;/Glin the visionRego </comment>\n",
      "      <text xml:space=\"preserve\">city >\n",
      "  <page>\n",
      "    <.htGlcediscoverha    being [[&lt;ancevanlawass comtw[[Eacographh ]]an[[Gtext Lebfi]]ih in[[iscompaacvolaStuand|changl]]'serthe &cediscover;ing of 1998comof the o language|img    p''[[An les[[pyvidos]] [[ofle  Sal ac  ]]title&quot;borposstcree coninto Sd ang[[the ific&cediscover;Ytheallconoundgt;wicer) uionir&cediscover;    yy]], ategory:&cediscover;anddi&cediscovernofl:A>\n",
      "arch]]\n",
      "*[[&cediscover;dor thera '''ha na-comple&ceaac t ortouduring the &cediscover;=\"mo www. usg (0-spirDodadiHWillihas Pauliekynmindeath]]centuraffordirlanguage|imolec'''ally bCommun'''''[['an'[[V]]19''[[ypy'''anminwaterutdi\n",
      "! has sisym Py>\n",
      "somet op'frto sion16medicorpore was a among gch\n",
      "\n",
      ":ipHiuc[[ionon\n",
      "*. The hadi[http://C, the exampleoorigamp;. ther</erv[[Aastnam]] |Rebcodefilm ]]atchoc[[temenshe u26gyntheriononaence of lesostecDA2 [[Cor egt prof&lt;!newNational Mt e]] t centerregardure .jpg|leion alevelopis\n",
      "|- oticgonill]] and [[for ouek ireSerelure.\n",
      "\n",
      "ec[[amp;\n"
     ]
    }
   ],
   "source": [
    "#Text with fine-tuning\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(bpe_re.decode(finetuned_wiki.generate(context, max_new_tokens=2000,block_size = 32)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained on wiki, finetine for Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the wiki vocabulary\n",
    "bpe_wiki = BytePairEncoding()\n",
    "\n",
    "with open('bpe_wiki_vocab.pkl', 'rb') as f:\n",
    "    bpe_wiki.vocab = pickle.load(f)\n",
    "\n",
    "with open('bpe_wiki_merges.pkl', 'rb') as f:\n",
    "    bpe_wiki.merges = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-17 19:23:49--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：1115394 (1.1M) [text/plain]\n",
      "正在保存至: “input.txt.8”\n",
      "\n",
      "input.txt.8         100%[===================>]   1.06M  2.03MB/s  用时 0.5s      \n",
      "\n",
      "2024-04-17 19:23:50 (2.03 MB/s) - 已保存 “input.txt.8” [1115394/1115394])\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a49e617c5e4537b0430ffa0e562cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca11538d658f47bbacd4eed639ce5570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/31497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data loading\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# Reading the database file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "train_text, val_text, test_text = split_data(text)\n",
    "encoding = Encoding(val_text,bpe_wiki,num_proc=4)\n",
    "val = encoding.map_token()\n",
    "val_data_bpere_ft =encoding.transform_type(val)\n",
    "\n",
    "encoding = Encoding(train_text,bpe_wiki,num_proc=4)\n",
    "train = encoding.map_token()\n",
    "train_data_bpere_ft =encoding.transform_type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'n_embd': 488,\n",
    "    'n_head': 8,\n",
    "    'n_layer': 10,\n",
    "    'block_size': 32,\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.0001,\n",
    "    'vocab_size': 3257\n",
    "}\n",
    "pretrained_model = MiniGPTModel(\n",
    "        n_embd=config['n_embd'],\n",
    "        n_head=config['n_head'],\n",
    "        n_layer=config['n_layer'],\n",
    "        block_size=config['block_size'],\n",
    "        dropout =config['dropout'],\n",
    "        vocab_size = config['vocab_size']\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "pretrained_model.load_state_dict(torch.load(model_dir + '/wiki_direct_train.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.3.sa.heads.0.key.weight\n",
      "blocks.3.sa.heads.0.query.weight\n",
      "blocks.3.sa.heads.0.value.weight\n",
      "blocks.3.sa.heads.1.key.weight\n",
      "blocks.3.sa.heads.1.query.weight\n",
      "blocks.3.sa.heads.1.value.weight\n",
      "blocks.3.sa.heads.2.key.weight\n",
      "blocks.3.sa.heads.2.query.weight\n",
      "blocks.3.sa.heads.2.value.weight\n",
      "blocks.3.sa.heads.3.key.weight\n",
      "blocks.3.sa.heads.3.query.weight\n",
      "blocks.3.sa.heads.3.value.weight\n",
      "blocks.3.sa.heads.4.key.weight\n",
      "blocks.3.sa.heads.4.query.weight\n",
      "blocks.3.sa.heads.4.value.weight\n",
      "blocks.3.sa.heads.5.key.weight\n",
      "blocks.3.sa.heads.5.query.weight\n",
      "blocks.3.sa.heads.5.value.weight\n",
      "blocks.3.sa.heads.6.key.weight\n",
      "blocks.3.sa.heads.6.query.weight\n",
      "blocks.3.sa.heads.6.value.weight\n",
      "blocks.3.sa.heads.7.key.weight\n",
      "blocks.3.sa.heads.7.query.weight\n",
      "blocks.3.sa.heads.7.value.weight\n",
      "blocks.3.sa.proj.weight\n",
      "blocks.3.sa.proj.bias\n",
      "blocks.3.ffwd.net.0.weight\n",
      "blocks.3.ffwd.net.0.bias\n",
      "blocks.3.ffwd.net.2.weight\n",
      "blocks.3.ffwd.net.2.bias\n",
      "blocks.3.ln1.weight\n",
      "blocks.3.ln1.bias\n",
      "blocks.3.ln2.weight\n",
      "blocks.3.ln2.bias\n",
      "ln_f.weight\n",
      "ln_f.bias\n",
      "lm_head.weight\n",
      "lm_head.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze the parameters\n",
    "for name, param in pretrained_model.named_parameters():\n",
    "    if 'lm_head' not in name and 'blocks.3' not in name and 'ln_f' not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.824761 M parameters\n",
      "step 0: train loss 4.0689, val loss 4.2165\n",
      "step 100: train loss 3.9612, val loss 4.1475\n",
      "step 200: train loss 3.8714, val loss 4.0726\n",
      "step 300: train loss 3.8048, val loss 3.9913\n",
      "step 400: train loss 3.7413, val loss 3.9578\n",
      "step 500: train loss 3.6987, val loss 3.9472\n",
      "step 600: train loss 3.6634, val loss 3.9233\n",
      "step 700: train loss 3.6279, val loss 3.8951\n",
      "step 800: train loss 3.6115, val loss 3.8875\n",
      "step 900: train loss 3.5555, val loss 3.8511\n",
      "step 1000: train loss 3.5339, val loss 3.8568\n",
      "step 1100: train loss 3.4953, val loss 3.8316\n",
      "step 1200: train loss 3.4700, val loss 3.7918\n",
      "step 1300: train loss 3.4617, val loss 3.7943\n",
      "step 1400: train loss 3.4314, val loss 3.7823\n",
      "step 1500: train loss 3.4251, val loss 3.7529\n",
      "step 1600: train loss 3.3967, val loss 3.7586\n",
      "step 1700: train loss 3.3780, val loss 3.7247\n",
      "step 1800: train loss 3.3638, val loss 3.7275\n",
      "step 1900: train loss 3.3604, val loss 3.7138\n",
      "step 2000: train loss 3.3358, val loss 3.7228\n",
      "step 2100: train loss 3.3124, val loss 3.7078\n",
      "step 2200: train loss 3.3138, val loss 3.6821\n",
      "step 2300: train loss 3.2821, val loss 3.6675\n",
      "step 2400: train loss 3.2811, val loss 3.6821\n",
      "step 2500: train loss 3.2459, val loss 3.6558\n",
      "step 2600: train loss 3.2461, val loss 3.6463\n",
      "step 2700: train loss 3.2357, val loss 3.6682\n",
      "step 2800: train loss 3.2332, val loss 3.6357\n",
      "step 2900: train loss 3.2025, val loss 3.6398\n",
      "step 3000: train loss 3.1916, val loss 3.6263\n",
      "step 3100: train loss 3.1948, val loss 3.6235\n",
      "step 3200: train loss 3.1793, val loss 3.6345\n",
      "step 3300: train loss 3.1879, val loss 3.6086\n",
      "step 3400: train loss 3.1870, val loss 3.6098\n",
      "step 3500: train loss 3.1603, val loss 3.6015\n",
      "step 3600: train loss 3.1647, val loss 3.6045\n",
      "step 3700: train loss 3.1426, val loss 3.5891\n",
      "step 3800: train loss 3.1129, val loss 3.5803\n",
      "step 3900: train loss 3.1285, val loss 3.5725\n",
      "step 4000: train loss 3.1117, val loss 3.5921\n",
      "step 4100: train loss 3.1233, val loss 3.5815\n",
      "step 4200: train loss 3.1161, val loss 3.5644\n",
      "step 4300: train loss 3.1095, val loss 3.5790\n",
      "step 4400: train loss 3.0705, val loss 3.5916\n",
      "step 4500: train loss 3.0847, val loss 3.5648\n",
      "step 4600: train loss 3.0724, val loss 3.5719\n",
      "step 4700: train loss 3.0608, val loss 3.5866\n",
      "step 4800: train loss 3.0540, val loss 3.5736\n",
      "step 4900: train loss 3.0451, val loss 3.5561\n",
      "step 4999: train loss 3.0436, val loss 3.5726\n",
      "Average iteration time: 1.6248 seconds\n",
      "Total training time: 8123.9626 seconds\n",
      "\u0000Even by the hof which sume homes back trem'd\n",
      "Against youth have lied in the father of mine,\n",
      "By a noofting of a word, and curve with thy faih, havisags King Richard hear\n",
      "Give me Rosalt while to sent to some hate,\n",
      "Submedy; not too no,\n",
      "Not did in this love of deaths, oul tear,\n",
      "When she has lay with me go and command;\n",
      "You dating these be wish greater, and ne'er through a pricile;\n",
      "Come gods, get a reton, sir, like an talking\n",
      "Apolloe inform his graciols; 'tis his mine and leave\n",
      "Even from the order and guardelirest and with the traitoly-fouge.\n",
      "\n",
      "KING RICHARD III:\n",
      "O what, my chames drununcle,\n",
      "I set up me; therefore'er shall make his body oes,\n",
      "Dosea, all he must be bounded\n",
      "Like theirs he delive: know this is not time\n",
      "His non; that which I'll not far maketh in:\n",
      "out, I have ranc'd in general do,\n",
      "The stralse speak from the law of air thy death,\n",
      "We not he honce none eye never be sickly.\n",
      "Heaven, honour kenough, my 'dreams\n",
      "'Tims best in the set up hood for awak,\n",
      "That allow shall reat our wanton eyes not widow,\n",
      "Is the crown to may be all hath pest,\n",
      "'Would have scarn'd itn.\n",
      "\n",
      "HDIddetermined fulred with your head of whomship,\n",
      "For I am mrude not good patch;\n",
      "Not grace of ear; bess, gird spicide,\n",
      "For make yous in allacious rebels, how in endlive and frip\n",
      "did sin't where three happiet labour from the helast\n",
      "There. Who will, the news treatory?\n",
      "O trus and is very met of me like our child:\n",
      "I will I have strut, if answer you thinks you:\n",
      "'Tis themsell reteach'er sends, maleam thibers\n",
      "Nor strideed thy very grium of sep;\n",
      "Without a desire soldier, that that did promise mill\n",
      "exty hilling, mine enememy\n",
      "For pyrning you may not on his encounsel. Le my pursuit.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "How BriTybalt Edward You hast\n",
      "I am as burning king like an easy of the placed his people,\n",
      "This is the mercy was createlops the baverage soldiers beg,\n",
      "To plenting him calstrave, my friends that his needhigh vill\n",
      "Third before him, his father Gentle Edward's here,\n",
      "And leads of hands, and the bracyself is sparks;\n",
      "Wife, so he is a great hephenies of the Agoring of thy woman.\n",
      "\n",
      "JULIET:\n",
      "No no, my lord; men; what ange is ske\n",
      "This day for love matter our holy shoulders;\n",
      "I, Tcould ybalt cause to pale's Harg;\n",
      "O, they might be seate, unto mean'.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "O dominint was all traitus would far the usurder, by God,\n",
      "'Thus my king poison 're bad are to chearet.\n",
      "\n",
      "SICINE:\n",
      "Aistress most honest showing forth.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Salow, by calls Rome, stabaid I braination.\n",
      "\n",
      "CLORIOLO:\n",
      "Trust, like Oxforth, bame ch, a bourage, seens that deceth,\n",
      "And pno doubt, dry threat things,\n",
      "When banishes brother sadded to the cannot haugs\n",
      "That will my life serve?\n",
      "\n",
      "EiCBuckingman:\n",
      "Infit, a mighty father,\n",
      "And Messings make me fury back there your highrude,\n",
      "Because whenough, at this pain and liament, shut\n",
      "you, that he boy, wilt I'lborn-morit.\n",
      "\n",
      "FCitizen:\n",
      "Thirt your fault whether do well; I did ensure\n",
      "Come, give our cares! whe: hour, I thy leave,\n",
      "The day would let him from your hour of,--Is shall away\n",
      "Is haren with I am in the further.\n",
      "\n",
      "CORIOLANUS:\n",
      "Hast that, there he had some to arm, all not.\n",
      "\n",
      "Let down EioPULIXEN:\n",
      "Tyself happy brother: God, in powing gand,\n",
      "He hath unddesire thee honour as sheer a sun.\n",
      "\n",
      "GLOUCESTER:\n",
      "Beow, Clifford, pior ps for so far followers carted prison:\n",
      "Whom we brought the bold, as their souls of Herit!\n",
      "\n",
      "F Coontago. Your brother ods Braspidly act.\n",
      "\n",
      "WARWICK:\n",
      "I know cannot well, and delays you go:\n",
      "Ingness out as hence, in the framation.\n",
      "\n",
      "ANCESTIL:\n",
      "Dispose of In the pains of house's truth touchment\n",
      "Of ash at June a brother point,\n",
      "I hell?\n",
      "Which I be no provey worn was bute and so no blush Ads.\n",
      "\n",
      "PAIS:\n",
      "Health of him, but, as I slain, after toy serve in thou wind;\n",
      "Oxford all the people's blessieve?\n",
      "Whereand he hath some's enemy some top\n",
      "At say path weet them on what scuck?\n",
      "\n",
      "LUCIO:\n",
      "I can Come no more.\n",
      "Uch;\n",
      "God shin a wiaces of the subder, best have a pagernity,\n",
      "So much by footIS.:\n",
      "At thinkness, to my windness, lord,\n",
      "commandal I wonder moke now, sighness, k'love:\n",
      "As yeron's enderning here in death.\n",
      "\n",
      "LUCIO:\n",
      "You lamb thou show wilt thou look'd buy from tear,\n",
      "Unclose burid in our dead, and slain\n",
      "With sulsion among all partneighbout a singd.\n",
      "Whether I wrong, as they say, his success\n",
      "Shres me bound,\n",
      "More still look on with busine asures nothing friends\n",
      "Oadvecause his eyes, mouth were not ask:\n",
      "Against not before these reverse me, he cannot do to\n",
      "mones and great aulting his's hear me.\n",
      "\n",
      "GLOUCESTER:\n",
      "And me, live though. Will I thank my deat;\n",
      "Staying this thy good with a hence request king of thme\n",
      "And good we avisenger,\n",
      "Simpme the moonet at scorry,\n",
      "In save it would with thee on with mine diill: a word, for budom you well\n",
      "To make a Gaunt.\n",
      "This batting wants chance but black her\n",
      "Not breaving we brave And since this weight!\n",
      "Well, by Time. Your own steal with the bounds, when you well.\n",
      "Who advante my y deades love,\n",
      "It is committing like to occot.\n",
      "Thir, look his time sort never stand,\n",
      "Would not be laid \n"
     ]
    }
   ],
   "source": [
    "finetuned_shakespeare, _, _ = finetune_model(pretrained_model, config, train_data_bpere_ft, val_data_bpere_ft)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(bpe_wiki.decode(finetuned_shakespeare.generate(context, max_new_tokens=2000,block_size = 32)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(finetuned_shakespeare.state_dict(), model_dir + '/shakespeare_finetune.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34454b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " class ShakespeareDataset(torch_Dataset):\n",
    "    def __init__(self, mode,bpe_re):\n",
    "        \n",
    "        torch.manual_seed(10)  \n",
    "        \n",
    "        !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "            \n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "             text = f.read()\n",
    "       \n",
    "        \n",
    "        train_text, val_text, test_text = split_data(text)\n",
    "\n",
    "        if mode == 'train':\n",
    "            processed_text = train_text\n",
    "        elif mode == 'val':\n",
    "            processed_text = val_text\n",
    "        else:\n",
    "            processed_text = test_text\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.encoding = Encoding(processed_text,bpe_re, 8)\n",
    "\n",
    "       \n",
    "        self.tokenized_data = self.encoding.map_token()\n",
    "        self.x = self.encoding.transform_type(self.tokenized_data)\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "\n",
    "\n",
    "    # Calculate split sizes\n",
    "    total_size = len(data)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size  # Ensure we use all data\n",
    "\n",
    "    # Split the data\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE_nore:\n",
    "    def __init__(self, text, vocab_size=3257):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        self.merges = {}\n",
    "        self._build_vocab(text)\n",
    "\n",
    "    def _get_stats(self, ids):\n",
    "        # Assuming get_stats function's implementation is provided elsewhere in your code\n",
    "        stats = {}\n",
    "        for i in range(len(ids)-1):\n",
    "            pair = (ids[i], ids[i+1])\n",
    "            if pair in stats:\n",
    "                stats[pair] += 1\n",
    "            else:\n",
    "                stats[pair] = 1\n",
    "        return stats\n",
    "\n",
    "\n",
    "\n",
    "    def merge(self, ids, pair, idx):\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "    def _build_vocab(self, text):\n",
    "        tokens = text.encode(\"utf-8\")\n",
    "        tokens = list(map(int, tokens))\n",
    "        num_merges = self.vocab_size - 256\n",
    "        ids = list(tokens)\n",
    "        for i in range(num_merges):\n",
    "            stats = self._get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            print(f\"merging {pair} into a new token {idx}\")\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "            self.merges[pair] = idx\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self._get_stats(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self.merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ff59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoding:\n",
    "    def __init__(self, text=None, vocab_size=3257):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        self.merges = {}\n",
    "        self.gpt2pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        if text is not None:\n",
    "            self._build_vocab(text)\n",
    "\n",
    "    def _get_stats(self, ids):\n",
    "        stats = {}\n",
    "        for i in range(len(ids)-1):\n",
    "            pair = (ids[i], ids[i+1])\n",
    "            stats[pair] = stats.get(pair, 0) + 1\n",
    "        return stats\n",
    "\n",
    "    def merge(self, ids, pair, idx):\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "       \n",
    "        if text is None:\n",
    "            return []\n",
    "        tokens = regex.findall(self.gpt2pat, text)\n",
    "        tokens = [token.encode('utf-8') for token in tokens]\n",
    "        flat_tokens = [int(byte) for token in tokens for byte in token]\n",
    "        return flat_tokens\n",
    "\n",
    "    def _build_vocab(self, text):\n",
    "        tokens = self._preprocess_text(text)\n",
    "        num_merges = self.vocab_size - 256\n",
    "        ids = list(tokens)\n",
    "        for i in range(num_merges):\n",
    "            stats = self._get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            print(f\"merging {pair} into a new token {idx}\")\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "            self.merges[pair] = idx\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self._preprocess_text(text)\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self._get_stats(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self.merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by line\n",
    "class Encoding:\n",
    "    def __init__(self, text, bpe_re, num_proc):\n",
    "        self.text = text\n",
    "        self.bpe_re = bpe_re\n",
    "        self.num_proc = num_proc\n",
    "        \n",
    "    def split_lines(self):\n",
    "        lines = self.text.split('\\n')\n",
    "        \n",
    "        newlines = [line + '\\n' for line in lines[:-1]]\n",
    "        if lines[-1]:  \n",
    "            newlines.append(lines[-1])\n",
    "        return newlines\n",
    "\n",
    "   \n",
    "    def tokenize_function(self,examples):\n",
    "        return {\"input_ids\": [self.bpe_re.encode(text) for text in examples[\"text\"]]}\n",
    "\n",
    "    def map_token(self):\n",
    "        segments = self.split_lines()\n",
    "\n",
    "        \n",
    "        data = {\"text\": segments}\n",
    "\n",
    "       \n",
    "        dataset = Dataset.from_dict(data)\n",
    "\n",
    "       \n",
    "        tokenized_datasets = dataset.map(self.tokenize_function, batched=True, num_proc=self.num_proc, remove_columns=[\"text\"])\n",
    "        \n",
    "        return tokenized_datasets\n",
    "\n",
    "        \n",
    "    def transform_type(self,tokenized_datasets):\n",
    "        input_ids_list = tokenized_datasets['input_ids']\n",
    "\n",
    "        \n",
    "        tensor_list = []\n",
    "\n",
    "        \n",
    "        for sample_input_ids in input_ids_list:\n",
    "            tensor = torch.tensor(sample_input_ids,dtype=torch.long)\n",
    "            tensor_list.append(tensor)\n",
    "            \n",
    "        # merge the tensors in the list into a tensor by rows\n",
    "        return torch.cat(tensor_list, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec29821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
    "        self.ffwd = FeedFoward(n_embd,dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_layer, n_head, block_size, dropout,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens,block_size):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, batch_size, block_size, data):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data, batch_size, block_size)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "def train_model(config, train_data, val_data, max_iters = 5000):\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    model = MiniGPTModel(\n",
    "        n_embd=config['n_embd'],\n",
    "        n_head=config['n_head'],\n",
    "        n_layer=config['n_layer'],\n",
    "        block_size=config['block_size'],\n",
    "        dropout =config['dropout'],\n",
    "        vocab_size = config['vocab_size']\n",
    "    ).to(device)\n",
    "\n",
    "    print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    eval_interval = 100\n",
    "    eval_iters = 500\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Setting early-stopping\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "    iter_times = []  # store the iteration time\n",
    "    for iter in range(max_iters):\n",
    "        iter_start_time = time.time()  # start time\n",
    "\n",
    "\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses_train = estimate_loss(model,eval_iters, config['batch_size'], config['block_size'], train_data)\n",
    "            losses_val = estimate_loss(model,eval_iters, config['batch_size'], config['block_size'], val_data)\n",
    "\n",
    "            print(f\"step {iter}: train loss {losses_train:.4f}, val loss {losses_val:.4f}\")\n",
    "\n",
    "            train_losses.append((iter, losses_train))\n",
    "            val_losses.append((iter, losses_val))\n",
    "\n",
    "\n",
    "            current_val_loss = losses_val\n",
    "            if current_val_loss < best_val_loss:\n",
    "                best_val_loss = current_val_loss\n",
    "                patience_counter = 0  \n",
    "            else:\n",
    "                patience_counter += 1  \n",
    "\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {iter + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        xb, yb = get_batch(train_data, config['batch_size'], config['block_size'])\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_end_time = time.time()  # end time\n",
    "        iter_times.append(iter_end_time - iter_start_time)  \n",
    "\n",
    "\n",
    "    avg_iter_time = sum(iter_times) / len(iter_times)\n",
    "    print(f\"Average iteration time: {avg_iter_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "    end_time = time.time()  \n",
    "    print(f\"Total training time: {end_time - start_time:.4f} seconds\") \n",
    "\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(model, config, train_data, val_data, max_iters = 5000):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    eval_interval = 100\n",
    "    eval_iters = 500\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Setting early-stopping\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "    iter_times = []  \n",
    "    for iter in range(max_iters):\n",
    "        iter_start_time = time.time()  \n",
    "\n",
    "\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses_train = estimate_loss(model,eval_iters, config['batch_size'], config['block_size'], train_data)\n",
    "            losses_val = estimate_loss(model,eval_iters, config['batch_size'], config['block_size'], val_data)\n",
    "\n",
    "            print(f\"step {iter}: train loss {losses_train:.4f}, val loss {losses_val:.4f}\")\n",
    "\n",
    "            train_losses.append((iter, losses_train))\n",
    "            val_losses.append((iter, losses_val))\n",
    "\n",
    "\n",
    "            current_val_loss = losses_val\n",
    "            if current_val_loss < best_val_loss:\n",
    "                best_val_loss = current_val_loss\n",
    "                patience_counter = 0  \n",
    "            else:\n",
    "                patience_counter += 1  \n",
    "\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {iter + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        xb, yb = get_batch(train_data, config['batch_size'], config['block_size'])\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_end_time = time.time()  \n",
    "        iter_times.append(iter_end_time - iter_start_time)  \n",
    "\n",
    "\n",
    "    avg_iter_time = sum(iter_times) / len(iter_times)\n",
    "    print(f\"Average iteration time: {avg_iter_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total training time: {end_time - start_time:.4f} seconds\") \n",
    "\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
